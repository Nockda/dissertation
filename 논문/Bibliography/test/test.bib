
@article{schulman_proximal_2017,
	title = {Proximal Policy Optimization Algorithms},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1707.06347},
	doi = {10.48550/ARXIV.1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization ({PPO}), have some of the benefits of trust region policy optimization ({TRPO}), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test {PPO} on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that {PPO} outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	urldate = {2023-08-11},
	date = {2017},
	note = {Publisher: {arXiv}
Version Number: 2},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
	file = {전문(全文):files/8/Schulman 등 - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf},
}

@article{mnih_asynchronous_2016,
	title = {Asynchronous Methods for Deep Reinforcement Learning},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1602.01783},
	doi = {10.48550/ARXIV.1602.01783},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core {CPU} instead of a {GPU}. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	urldate = {2023-08-11},
	date = {2016},
	note = {Publisher: {arXiv}
Version Number: 2},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
	file = {전문(全文):files/7/Mnih 등 - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:application/pdf},
}

@inproceedings{todorov_mujoco_2012,
	location = {Vilamoura-Algarve, Portugal},
	title = {{MuJoCo}: A physics engine for model-based control},
	isbn = {978-1-4673-1736-8 978-1-4673-1737-5 978-1-4673-1735-1},
	url = {http://ieeexplore.ieee.org/document/6386109/},
	doi = {10.1109/IROS.2012.6386109},
	shorttitle = {{MuJoCo}},
	abstract = {We describe a new physics engine tailored to model-based control. Multi-joint dynamics are represented in generalized coordinates and computed via recursive algorithms. Contact responses are computed via efﬁcient new algorithms we have developed, based on the modern velocity-stepping approach which avoids the difﬁculties with spring-dampers. Models are speciﬁed using either a high-level C++ {API} or an intuitive {XML} ﬁle format. A built-in compiler transforms the user model into an optimized data structure used for runtime computation. The engine can compute both forward and inverse dynamics. The latter are well-deﬁned even in the presence of contacts and equality constraints. The model can include tendon wrapping as well as actuator activation states (e.g. pneumatic cylinders or muscles). To facilitate optimal control applications and in particular sampling and ﬁnite differencing, the dynamics can be evaluated for different states and controls in parallel. Around 400,000 dynamics evaluations per second are possible on a 12-core machine, for a 3D homanoid with 18 dofs and 6 active contacts. We have already used the engine in a number of control applications. It will soon be made publicly available.},
	eventtitle = {2012 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS} 2012)},
	pages = {5026--5033},
	booktitle = {2012 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
	publisher = {{IEEE}},
	author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
	urldate = {2023-08-11},
	date = {2012-10},
	langid = {english},
	file = {Todorov 등 - 2012 - MuJoCo A physics engine for model-based control.pdf:files/4/Todorov 등 - 2012 - MuJoCo A physics engine for model-based control.pdf:application/pdf},
}
