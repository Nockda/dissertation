{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import shape,math\n",
    "from tensorflow.keras import Input,layers,Model\n",
    "from tensorflow.keras.losses import mse,binary_crossentropy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import torch; torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# output_dir = os.path.join(\".\", \"output\")  # Path to the output directory\n",
    "# subdirs = [f.path for f in os.scandir(output_dir) if f.is_dir()]\n",
    "# subdirs.sort()\n",
    "\n",
    "# # Create an empty 3D array to store the combined data\n",
    "# combined_arr = np.empty((len(subdirs), 999, 10))\n",
    "\n",
    "# # Loop through each subdirectory and load the CSV files\n",
    "# for i, subdir in enumerate(subdirs):\n",
    "#     action_filename = os.path.join(subdir, \"action.csv\")\n",
    "#     obs_filename = os.path.join(subdir, \"obs.csv\")\n",
    "\n",
    "#     # Load the action and obs CSV files\n",
    "#     action_df = pd.read_csv(action_filename)\n",
    "#     obs_df = pd.read_csv(obs_filename)\n",
    "\n",
    "#     # Concatenate the DataFrames horizontally\n",
    "#     combined_data = pd.concat([action_df, obs_df], axis=1)\n",
    "\n",
    "#     # Convert combined_data to a 3D array and assign it to combined_arr\n",
    "#     combined_arr[i] = np.reshape(combined_data.values, (999, 10))\n",
    "\n",
    "# # Print the shape of combined_arr\n",
    "# print(combined_arr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattened_arr = combined_arr.reshape(10000, 9990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattened_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "combined_df = np.array(combined_arr)\n",
    "combined_tensor = torch.from_numpy(combined_df)\n",
    "flattened_tensor = combined_tensor.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename dimensions to 'Seed' and 'Data'\n",
    "# flattened_tensor.names = (\"Seed\", \"Data\")\n",
    "\n",
    "# # Now flattened_tensor will have dimensions named 'Seed' and 'Data'\n",
    "# print(flattened_tensor.shape)  # Output: torch.Size([10000, 9990])\n",
    "# print(flattened_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(flattened_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000, 10)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "output_dir = os.path.join(\".\", \"output\")  # Path to the output directory\n",
    "subdirs = [f.path for f in os.scandir(output_dir) if f.is_dir()]\n",
    "subdirs.sort()\n",
    "\n",
    "# Create an empty 3D array to store the combined data\n",
    "combined_arr = np.empty((len(subdirs), 1000, 10))\n",
    "\n",
    "# Loop through each subdirectory and load the CSV files\n",
    "for i, subdir in enumerate(subdirs):\n",
    "    action_filename = os.path.join(subdir, \"action.csv\")\n",
    "    obs_filename = os.path.join(subdir, \"obs.csv\")\n",
    "\n",
    "    # Load the action and obs CSV files\n",
    "    action_df = pd.read_csv(action_filename,  header=None)\n",
    "    obs_df = pd.read_csv(obs_filename,  header=None)\n",
    "\n",
    "    # Concatenate the DataFrames horizontally\n",
    "    combined_data = pd.concat([action_df, obs_df], axis=1)\n",
    "\n",
    "    # Convert combined_data to a 3D array and assign it to combined_arr\n",
    "    combined_arr[i-1] = np.reshape(combined_data.values, (1000, 10))\n",
    "\n",
    "# Print the shape of combined_arr\n",
    "print(combined_arr.shape)\n",
    "flattened_arr = combined_arr.reshape(10000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1000, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Convert combined_arr to PyTorch Tensor\n",
    "combined_tensor = torch.from_numpy(combined_arr)\n",
    "\n",
    "# Print the shape of combined_tensor\n",
    "print(combined_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "# output_dir = os.path.join(\".\", \"output\")  # Path to the output directory\n",
    "# subdirs = [f.path for f in os.scandir(output_dir) if f.is_dir()]\n",
    "# combined_df = pd.DataFrame()\n",
    "\n",
    "# # Create a tqdm progress bar\n",
    "# pbar = tqdm(subdirs, desc=\"Processing subdirectories\", unit=\"subdir\")\n",
    "\n",
    "# for subdir in pbar:\n",
    "#     action_filename = os.path.join(subdir, \"action.csv\")\n",
    "#     obs_filename = os.path.join(subdir, \"obs.csv\")\n",
    "#     action_df = pd.read_csv(action_filename)\n",
    "#     obs_df = pd.read_csv(obs_filename)\n",
    "#     combined_data = pd.concat([action_df, obs_df], axis=1)\n",
    "#     if combined_data.shape == (999,10):\n",
    "#         combined_df = pd.concat([combined_df, combined_data], ignore_index=True)\n",
    "#     else:\n",
    "#         print(\"fail : \", subdir)\n",
    "\n",
    "#     # Update the progress bar\n",
    "#     pbar.set_postfix({\"Processed subdirectories\": subdir})\n",
    "\n",
    "# # Close the progress bar\n",
    "# pbar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1000, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Convert combined_arr to PyTorch Tensor\n",
    "combined_tensor = torch.from_numpy(combined_arr)\n",
    "\n",
    "# Print the shape of combined_tensor\n",
    "print(combined_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE code -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# Assuming combined_tensor is your data\n",
    "# Convert the data to float32\n",
    "dataset = TensorDataset(combined_tensor.float())\n",
    "\n",
    "# Define the data loader\n",
    "batch_size = 256  # adjust as necessary\n",
    "\n",
    "# Split data into train, validation, and test\n",
    "train_size = int(0.7 * len(dataset))  # 70% for training\n",
    "valid_size = int(0.15 * len(dataset))  # 15% for validation\n",
    "test_size = len(dataset) - train_size - valid_size  # 15% for testing\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Hyperparameters\n",
    "input_shape = combined_tensor.shape[1] * combined_tensor.shape[2]  # modify this to match your data\n",
    "hidden_dim1 = 128  # modify as needed\n",
    "hidden_dim2 = 256  # modify as needed\n",
    "hidden_dim3 = 512  # modify as needed\n",
    "latent_dim = 2  # modify as needed\n",
    "lr = 1e-3  # learning rate\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_shape, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim3, latent_dim * 2)  # mean and variance\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim3, input_shape),\n",
    "#             nn.Sigmoid()      ## 시그모이드를 뺴야됨. 왜냐면 애초데이터가 0과 1사이가 아니기때문\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = h.chunk(2, dim=1)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, mu, log_var\n",
    "    \n",
    "# Model, optimizer, and loss function\n",
    "model = VAE(input_shape, hidden_dim1, latent_dim)\n",
    "\n",
    "optimizer = optim.NAdam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss() #MSE로 바꿔보자.\n",
    "\n",
    "def train(epoch, model, optimizer, loss_fn, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kl_div = 0\n",
    "    for i, batch in enumerate(train_loader):  # using train_loader instead of dataloader\n",
    "        batch_data = batch[0]  # get the data from the batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Flatten the data\n",
    "        batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "\n",
    "        reconstructed_batch, mu, log_var = model(batch_data)\n",
    "\n",
    "        # Loss: reconstruction loss + KL divergence\n",
    "        recon_loss = loss_fn(reconstructed_batch, batch_data)\n",
    "        kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        loss = recon_loss + kl_divergence\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_kl_div += kl_divergence.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_div = total_kl_div / len(train_loader.dataset)\n",
    "    print(f'====> Epoch: {epoch} Average loss: {avg_loss}, Recon Loss: {avg_recon_loss}, KL Div: {avg_kl_div}')\n",
    "\n",
    "    return avg_loss, avg_recon_loss, avg_kl_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lists to store losses for each epoch\n",
    "avg_losses = []\n",
    "avg_recon_losses = []\n",
    "avg_kl_divs = []\n",
    "\n",
    "# Training\n",
    "n_epochs = 50  # modify as needed\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    avg_loss, avg_recon_loss, avg_kl_div = train(epoch, model, optimizer, loss_fn, train_loader)\n",
    "    avg_losses.append(avg_loss)\n",
    "    avg_recon_losses.append(avg_recon_loss)\n",
    "    avg_kl_divs.append(avg_kl_div)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(avg_losses, label='Average Loss')\n",
    "plt.plot(avg_recon_losses, label='Reconstruction Loss')\n",
    "plt.plot(avg_kl_divs, label='KL Divergence')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE code -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 0.005870787484305246, Recon Loss: 0.005667482035500663, KL Div: 0.0010165273972920009\n",
      "====> Epoch: 2 Average loss: 0.005738260098866054, Recon Loss: 0.00566685152053833, KL Div: 0.000357042806489127\n",
      "====> Epoch: 3 Average loss: 0.005703141348702567, Recon Loss: 0.005665932553155082, KL Div: 0.0001860439692224775\n",
      "====> Epoch: 4 Average loss: 0.00568770238331386, Recon Loss: 0.005665148394448417, KL Div: 0.00011277005076408386\n",
      "====> Epoch: 5 Average loss: 0.005680397033691407, Recon Loss: 0.0056643990448543005, KL Div: 7.999003784997123e-05\n",
      "====> Epoch: 6 Average loss: 0.00567561537878854, Recon Loss: 0.005663144418171474, KL Div: 6.23546200139182e-05\n",
      "====> Epoch: 7 Average loss: 0.0056727290834699354, Recon Loss: 0.005662572349820818, KL Div: 5.078362567084176e-05\n",
      "====> Epoch: 8 Average loss: 0.005670546701976231, Recon Loss: 0.005661786760602678, KL Div: 4.379941735948835e-05\n",
      "====> Epoch: 9 Average loss: 0.00566886636189052, Recon Loss: 0.00566123696735927, KL Div: 3.8147194044930595e-05\n",
      "====> Epoch: 10 Average loss: 0.005667097364153181, Recon Loss: 0.00566029327256339, KL Div: 3.4020547355924335e-05\n",
      "====> Epoch: 11 Average loss: 0.005666064807346889, Recon Loss: 0.005659973859786988, KL Div: 3.04548442363739e-05\n",
      "====> Epoch: 12 Average loss: 0.005665375982012067, Recon Loss: 0.00565985199383327, KL Div: 2.7620153767721995e-05\n",
      "====> Epoch: 13 Average loss: 0.005664820432662964, Recon Loss: 0.005659800563539778, KL Div: 2.5099417993000575e-05\n",
      "====> Epoch: 14 Average loss: 0.005663584402629308, Recon Loss: 0.005658939055034093, KL Div: 2.3226533617292132e-05\n",
      "====> Epoch: 15 Average loss: 0.005662736586162022, Recon Loss: 0.0056584619794573104, KL Div: 2.1373386893953595e-05\n",
      "====> Epoch: 16 Average loss: 0.005662325518471854, Recon Loss: 0.005658373764583043, KL Div: 1.9758777959006175e-05\n",
      "====> Epoch: 17 Average loss: 0.005661673784255981, Recon Loss: 0.00565800826890128, KL Div: 1.832759806088039e-05\n",
      "====> Epoch: 18 Average loss: 0.005660912649972098, Recon Loss: 0.0056574545928410125, KL Div: 1.7290392092296055e-05\n",
      "====> Epoch: 19 Average loss: 0.005660344226019723, Recon Loss: 0.005657094887324742, KL Div: 1.624685525894165e-05\n",
      "====> Epoch: 20 Average loss: 0.005659683874675206, Recon Loss: 0.005656623601913452, KL Div: 1.5301363808768137e-05\n",
      "====> Epoch: 21 Average loss: 0.005658644608088902, Recon Loss: 0.005655766453061785, KL Div: 1.4390553746904646e-05\n",
      "====> Epoch: 22 Average loss: 0.005658255236489432, Recon Loss: 0.005655511277062552, KL Div: 1.3719699212482998e-05\n",
      "====> Epoch: 23 Average loss: 0.005657374347959246, Recon Loss: 0.0056547611100333075, KL Div: 1.3066313096455165e-05\n",
      "====> Epoch: 24 Average loss: 0.0056565595354352675, Recon Loss: 0.005654071399143764, KL Div: 1.2440600565501622e-05\n",
      "====> Epoch: 25 Average loss: 0.005656056676592146, Recon Loss: 0.005653686319078718, KL Div: 1.1851817369461059e-05\n",
      "====> Epoch: 26 Average loss: 0.005654760565076556, Recon Loss: 0.005652489423751831, KL Div: 1.1355791773114885e-05\n",
      "====> Epoch: 27 Average loss: 0.005653749908719744, Recon Loss: 0.0056515706266675675, KL Div: 1.0896559272493634e-05\n",
      "====> Epoch: 28 Average loss: 0.005652510711124965, Recon Loss: 0.0056504256044115336, KL Div: 1.0425261088779995e-05\n",
      "====> Epoch: 29 Average loss: 0.005650974409920829, Recon Loss: 0.00564899366242545, KL Div: 9.903745991843087e-06\n",
      "====> Epoch: 30 Average loss: 0.005649898018155779, Recon Loss: 0.005647987229483468, KL Div: 9.553994451250349e-06\n",
      "====> Epoch: 31 Average loss: 0.0056485541207449775, Recon Loss: 0.005646714312689645, KL Div: 9.198976414544242e-06\n",
      "====> Epoch: 32 Average loss: 0.00564734673500061, Recon Loss: 0.005645546810967582, KL Div: 8.99976066180638e-06\n",
      "====> Epoch: 33 Average loss: 0.005645162718636649, Recon Loss: 0.005643449476787022, KL Div: 8.56637954711914e-06\n",
      "====> Epoch: 34 Average loss: 0.005643164089747838, Recon Loss: 0.0056415096010480606, KL Div: 8.27265637261527e-06\n",
      "====> Epoch: 35 Average loss: 0.005641373702457973, Recon Loss: 0.005639794928686959, KL Div: 7.893655981336322e-06\n",
      "====> Epoch: 36 Average loss: 0.005639286926814488, Recon Loss: 0.005637740646089826, KL Div: 7.731335503714425e-06\n",
      "====> Epoch: 37 Average loss: 0.005637270552771432, Recon Loss: 0.005635757378169469, KL Div: 7.566039051328387e-06\n",
      "====> Epoch: 38 Average loss: 0.005634381362370083, Recon Loss: 0.005632913146700178, KL Div: 7.340865475790841e-06\n",
      "====> Epoch: 39 Average loss: 0.005631774357386998, Recon Loss: 0.005630355017525809, KL Div: 7.096673761095319e-06\n",
      "====> Epoch: 40 Average loss: 0.0056296088695526126, Recon Loss: 0.005628237451825823, KL Div: 6.857514381408691e-06\n",
      "====> Epoch: 41 Average loss: 0.005627005134310041, Recon Loss: 0.005625704288482666, KL Div: 6.50419933455331e-06\n",
      "====> Epoch: 42 Average loss: 0.005623335497719901, Recon Loss: 0.005622091225215367, KL Div: 6.221528564180646e-06\n",
      "====> Epoch: 43 Average loss: 0.005621090820857457, Recon Loss: 0.005619846071515764, KL Div: 6.223767995834351e-06\n",
      "====> Epoch: 44 Average loss: 0.0056174464566367014, Recon Loss: 0.005616269554410662, KL Div: 5.8843706335340225e-06\n",
      "====> Epoch: 45 Average loss: 0.0056149914264678955, Recon Loss: 0.0056138462339128765, KL Div: 5.72581376348223e-06\n",
      "====> Epoch: 46 Average loss: 0.00561225836617606, Recon Loss: 0.00561111763545445, KL Div: 5.7040750980377195e-06\n",
      "====> Epoch: 47 Average loss: 0.005608760799680437, Recon Loss: 0.00560767810685294, KL Div: 5.413242748805454e-06\n",
      "====> Epoch: 48 Average loss: 0.005606478554861886, Recon Loss: 0.005605420487267631, KL Div: 5.290210247039795e-06\n",
      "====> Epoch: 49 Average loss: 0.005602531637464251, Recon Loss: 0.00560149427822658, KL Div: 5.187200648444039e-06\n",
      "====> Epoch: 50 Average loss: 0.005599180664334978, Recon Loss: 0.005598181281770979, KL Div: 4.996504102434431e-06\n",
      "====> Epoch: 51 Average loss: 0.005595672539302281, Recon Loss: 0.005594697373253958, KL Div: 4.875544990812029e-06\n",
      "====> Epoch: 52 Average loss: 0.005592352492468698, Recon Loss: 0.005591366154806954, KL Div: 4.931807518005371e-06\n",
      "====> Epoch: 53 Average loss: 0.005588300466537475, Recon Loss: 0.00558735956464495, KL Div: 4.704347678593227e-06\n",
      "====> Epoch: 54 Average loss: 0.005584900753838676, Recon Loss: 0.005583997998918806, KL Div: 4.513936383383615e-06\n",
      "====> Epoch: 55 Average loss: 0.005580310310636248, Recon Loss: 0.00557941814831325, KL Div: 4.460841417312622e-06\n",
      "====> Epoch: 56 Average loss: 0.005578601700919015, Recon Loss: 0.005577757256371634, KL Div: 4.221750157220023e-06\n",
      "====> Epoch: 57 Average loss: 0.005575200080871582, Recon Loss: 0.005574368987764631, KL Div: 4.155218601226806e-06\n",
      "====> Epoch: 58 Average loss: 0.00557218176977975, Recon Loss: 0.005571377038955688, KL Div: 4.023781844547817e-06\n",
      "====> Epoch: 59 Average loss: 0.005567724432264056, Recon Loss: 0.0055669339043753485, KL Div: 3.952886377062116e-06\n",
      "====> Epoch: 60 Average loss: 0.005564410379954747, Recon Loss: 0.005563618966511317, KL Div: 3.9569480078560966e-06\n",
      "====> Epoch: 61 Average loss: 0.005562591961451939, Recon Loss: 0.0055618070193699425, KL Div: 3.92450179372515e-06\n",
      "====> Epoch: 62 Average loss: 0.005560070276260376, Recon Loss: 0.005559318746839251, KL Div: 3.757481064115252e-06\n",
      "====> Epoch: 63 Average loss: 0.005555894136428833, Recon Loss: 0.005555129664284842, KL Div: 3.822211708341326e-06\n",
      "====> Epoch: 64 Average loss: 0.005551262651171003, Recon Loss: 0.005550531932285854, KL Div: 3.6535731383732385e-06\n",
      "====> Epoch: 65 Average loss: 0.0055492840835026334, Recon Loss: 0.005548585721424647, KL Div: 3.4916698932647705e-06\n",
      "====> Epoch: 66 Average loss: 0.005547474043709891, Recon Loss: 0.0055467815739767895, KL Div: 3.461965492793492e-06\n",
      "====> Epoch: 67 Average loss: 0.005543073517935617, Recon Loss: 0.005542399202074323, KL Div: 3.3717027732304163e-06\n",
      "====> Epoch: 68 Average loss: 0.005539583308356149, Recon Loss: 0.005538919108254569, KL Div: 3.3210856573922294e-06\n",
      "====> Epoch: 69 Average loss: 0.005535810164042881, Recon Loss: 0.005535156420298985, KL Div: 3.2689358506883896e-06\n",
      "====> Epoch: 70 Average loss: 0.0055327330657414025, Recon Loss: 0.005532119171960013, KL Div: 3.0696988105773924e-06\n",
      "====> Epoch: 71 Average loss: 0.00552994806425912, Recon Loss: 0.005529350927897862, KL Div: 2.98540507044111e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 72 Average loss: 0.005525164706366403, Recon Loss: 0.005524573462350028, KL Div: 2.956415925707136e-06\n",
      "====> Epoch: 73 Average loss: 0.0055240416867392405, Recon Loss: 0.0055234533718654085, KL Div: 2.941608428955078e-06\n",
      "====> Epoch: 74 Average loss: 0.005523856299264091, Recon Loss: 0.005523289271763393, KL Div: 2.835120473589216e-06\n",
      "====> Epoch: 75 Average loss: 0.005518317869731358, Recon Loss: 0.005517760004316057, KL Div: 2.789327076503209e-06\n",
      "====> Epoch: 76 Average loss: 0.005515802383422852, Recon Loss: 0.005515257188252041, KL Div: 2.7261972427368164e-06\n",
      "====> Epoch: 77 Average loss: 0.0055139424119676865, Recon Loss: 0.005513414587293352, KL Div: 2.6388934680393763e-06\n",
      "====> Epoch: 78 Average loss: 0.005508289541516985, Recon Loss: 0.0055077680519648965, KL Div: 2.6073881558009558e-06\n",
      "====> Epoch: 79 Average loss: 0.005507189920970372, Recon Loss: 0.005506669827869961, KL Div: 2.6002952030726843e-06\n",
      "====> Epoch: 80 Average loss: 0.005506388323647636, Recon Loss: 0.005505882092884609, KL Div: 2.5311836174556188e-06\n",
      "====> Epoch: 81 Average loss: 0.005503933600017003, Recon Loss: 0.005503415993281773, KL Div: 2.588033676147461e-06\n",
      "====> Epoch: 82 Average loss: 0.005500810248511178, Recon Loss: 0.005500312566757202, KL Div: 2.4883406502859933e-06\n",
      "====> Epoch: 83 Average loss: 0.005498409918376378, Recon Loss: 0.005497945989881243, KL Div: 2.3196169308253697e-06\n",
      "====> Epoch: 84 Average loss: 0.005495755536215646, Recon Loss: 0.005495259591511318, KL Div: 2.479621342250279e-06\n",
      "====> Epoch: 85 Average loss: 0.005494175059454782, Recon Loss: 0.005493708372116089, KL Div: 2.333317484174456e-06\n",
      "====> Epoch: 86 Average loss: 0.005490974392209734, Recon Loss: 0.0054905083520071845, KL Div: 2.330256359917777e-06\n",
      "====> Epoch: 87 Average loss: 0.005491737570081439, Recon Loss: 0.005491297006607056, KL Div: 2.202821629387992e-06\n",
      "====> Epoch: 88 Average loss: 0.005487784624099732, Recon Loss: 0.005487354210444859, KL Div: 2.152102334158761e-06\n",
      "====> Epoch: 89 Average loss: 0.0054858370167868474, Recon Loss: 0.005485404014587403, KL Div: 2.1646576268332344e-06\n",
      "====> Epoch: 90 Average loss: 0.00548165225982666, Recon Loss: 0.005481240953717913, KL Div: 2.056598663330078e-06\n",
      "====> Epoch: 91 Average loss: 0.0054865888868059435, Recon Loss: 0.005486159358705793, KL Div: 2.1476447582244874e-06\n",
      "====> Epoch: 92 Average loss: 0.005480786051068987, Recon Loss: 0.005480373791285923, KL Div: 2.061171191079276e-06\n",
      "====> Epoch: 93 Average loss: 0.005478414739881243, Recon Loss: 0.005478010552270072, KL Div: 2.02112112726484e-06\n",
      "====> Epoch: 94 Average loss: 0.0054758860383714945, Recon Loss: 0.005475506101335798, KL Div: 1.8996809210096087e-06\n",
      "====> Epoch: 95 Average loss: 0.005477553878511701, Recon Loss: 0.0054771652562277656, KL Div: 1.9433668681553433e-06\n",
      "====> Epoch: 96 Average loss: 0.0054757025923047745, Recon Loss: 0.005475299119949341, KL Div: 2.0172383104051865e-06\n",
      "====> Epoch: 97 Average loss: 0.00547475985118321, Recon Loss: 0.005474366630826678, KL Div: 1.9663614886147634e-06\n",
      "====> Epoch: 98 Average loss: 0.005471292734146118, Recon Loss: 0.005470924445561001, KL Div: 1.841719661440168e-06\n",
      "====> Epoch: 99 Average loss: 0.005470970664705549, Recon Loss: 0.005470610959189279, KL Div: 1.7985233238765171e-06\n",
      "====> Epoch: 100 Average loss: 0.005470080648149763, Recon Loss: 0.005469720125198365, KL Div: 1.802844660622733e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_shape, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim3, latent_dim * 2)  # mean and variance\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = h.chunk(2, dim=1)\n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_shape):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim3, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, output_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_shape, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim1, hidden_dim2, hidden_dim3, input_shape)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, mu, log_var\n",
    "\n",
    "    \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# Assuming combined_tensor is your data\n",
    "# Convert the data to float32\n",
    "dataset = TensorDataset(combined_tensor.float())\n",
    "\n",
    "# Define the data loader\n",
    "batch_size = 512  # adjust as necessary\n",
    "\n",
    "# Split data into train, validation, and test\n",
    "train_size = int(0.7 * len(dataset))  # 70% for training\n",
    "valid_size = int(0.15 * len(dataset))  # 15% for validation\n",
    "test_size = len(dataset) - train_size - valid_size  # 15% for testing\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Hyperparameters\n",
    "input_shape = combined_tensor.shape[1] * combined_tensor.shape[2]  # modify this to match your data\n",
    "hidden_dim1 = 24  # modify as needed\n",
    "hidden_dim2 = 128  # modify as needed\n",
    "hidden_dim3 = 512  # modify as needed\n",
    "latent_dim = 2  # modify as needed\n",
    "lr = 5e-5  # learning rate\n",
    "n_epochs = 100  # modify as needed\n",
    "beta = 0.2\n",
    "    \n",
    "# Model, optimizer, and loss function\n",
    "model = VAE(input_shape, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim)\n",
    "\n",
    "optimizer = optim.RAdam(model.parameters(), lr=lr)  # Make sure you're using the correct optimizer\n",
    "loss_fn = nn.MSELoss()  # And the correct loss function\n",
    "\n",
    "\n",
    "def train(epoch, model, optimizer, loss_fn, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kl_div = 0\n",
    "    for i, batch in enumerate(train_loader):  # using train_loader instead of dataloader\n",
    "        batch_data = batch[0]  # get the data from the batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Flatten the data\n",
    "        batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "\n",
    "        reconstructed_batch, mu, log_var = model(batch_data)\n",
    "\n",
    "        # Loss: reconstruction loss + KL divergence\n",
    "        recon_loss = loss_fn(reconstructed_batch, batch_data)\n",
    "        kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        loss = recon_loss + beta*kl_divergence\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_kl_div += kl_divergence.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_div = total_kl_div / len(train_loader.dataset)\n",
    "    print(f'====> Epoch: {epoch} Average loss: {avg_loss}, Recon Loss: {avg_recon_loss}, KL Div: {avg_kl_div}')\n",
    "\n",
    "    return avg_loss, avg_recon_loss, avg_kl_div\n",
    "\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lists to store losses for each epoch\n",
    "avg_losses = []\n",
    "avg_recon_losses = []\n",
    "avg_kl_divs = []\n",
    "\n",
    "# Training\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    avg_loss, avg_recon_loss, avg_kl_div = train(epoch, model, optimizer, loss_fn, train_loader)\n",
    "    avg_losses.append(avg_loss)\n",
    "    avg_recon_losses.append(avg_recon_loss)\n",
    "    avg_kl_divs.append(avg_kl_div)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(avg_losses, label='Average Loss')\n",
    "plt.plot(avg_recon_losses, label='Reconstruction Loss')\n",
    "plt.plot(avg_kl_divs, label='KL Divergence')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kl_div = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            batch_data = batch[0]  # get the data from the batch\n",
    "\n",
    "            # Flatten the data\n",
    "            batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "\n",
    "            reconstructed_batch, mu, log_var = model(batch_data)\n",
    "\n",
    "            # Loss: reconstruction loss + KL divergence\n",
    "            recon_loss = loss_fn(reconstructed_batch, batch_data)\n",
    "            kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "            loss += recon_loss + kl_divergence\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_kl_div += kl_divergence.item()\n",
    "\n",
    "    avg_loss = loss / len(dataloader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(dataloader.dataset)\n",
    "    avg_kl_div = total_kl_div / len(dataloader.dataset)\n",
    "\n",
    "    return avg_loss, avg_recon_loss, avg_kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0055), 0.005478294690450033, 4.403412342071533e-06)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, loss_fn, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            batch_data = batch[0]  # get the data from the batch\n",
    "            batch_data = batch_data.view(batch_data.size(0), -1)  # flatten the data\n",
    "            reconstructed_batch, _, _ = model(batch_data)  # get reconstructed data from the model\n",
    "\n",
    "            # Convert tensors to numpy arrays for use in pandas\n",
    "            original_data = batch_data.detach().cpu().numpy()\n",
    "            reconstructed_data = reconstructed_batch.detach().cpu().numpy()\n",
    "\n",
    "            # Convert to dataframes\n",
    "            original_df = pd.DataFrame(original_data)\n",
    "            reconstructed_df = pd.DataFrame(reconstructed_data)\n",
    "\n",
    "            if i == 0:  # for the first iteration, create the dataframes\n",
    "                all_original_df = original_df\n",
    "                all_reconstructed_df = reconstructed_df\n",
    "            else:  # for subsequent iterations, append to the existing dataframes\n",
    "                all_original_df = pd.concat([all_original_df, original_df])\n",
    "                all_reconstructed_df = pd.concat([all_reconstructed_df, reconstructed_df])\n",
    "    \n",
    "    return all_original_df, all_reconstructed_df\n",
    "\n",
    "# Call the function after training\n",
    "original_df, reconstructed_df = test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df, reconstructed_df = test_model(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9990</th>\n",
       "      <th>9991</th>\n",
       "      <th>9992</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011059</td>\n",
       "      <td>-0.000688</td>\n",
       "      <td>-0.050495</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>-0.078168</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>-0.020656</td>\n",
       "      <td>0.064324</td>\n",
       "      <td>-0.039187</td>\n",
       "      <td>-0.050386</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.014331</td>\n",
       "      <td>0.041990</td>\n",
       "      <td>0.062897</td>\n",
       "      <td>0.056659</td>\n",
       "      <td>0.083365</td>\n",
       "      <td>-0.010076</td>\n",
       "      <td>0.019453</td>\n",
       "      <td>0.042835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.045093</td>\n",
       "      <td>0.357293</td>\n",
       "      <td>-0.052318</td>\n",
       "      <td>0.023984</td>\n",
       "      <td>-0.079437</td>\n",
       "      <td>-0.064630</td>\n",
       "      <td>-0.177694</td>\n",
       "      <td>0.068332</td>\n",
       "      <td>-0.378153</td>\n",
       "      <td>0.679334</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.054762</td>\n",
       "      <td>-0.094758</td>\n",
       "      <td>-0.066864</td>\n",
       "      <td>-0.062517</td>\n",
       "      <td>0.042597</td>\n",
       "      <td>0.058450</td>\n",
       "      <td>-0.020841</td>\n",
       "      <td>-0.082203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.160763</td>\n",
       "      <td>-0.478253</td>\n",
       "      <td>-0.058969</td>\n",
       "      <td>0.097277</td>\n",
       "      <td>0.062118</td>\n",
       "      <td>0.049360</td>\n",
       "      <td>0.098586</td>\n",
       "      <td>0.029956</td>\n",
       "      <td>0.336544</td>\n",
       "      <td>-0.803452</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.026777</td>\n",
       "      <td>-0.089011</td>\n",
       "      <td>0.047432</td>\n",
       "      <td>0.010113</td>\n",
       "      <td>0.057112</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.036619</td>\n",
       "      <td>0.090570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.107175</td>\n",
       "      <td>-0.017778</td>\n",
       "      <td>-0.014382</td>\n",
       "      <td>-0.102119</td>\n",
       "      <td>-0.068903</td>\n",
       "      <td>0.077682</td>\n",
       "      <td>-0.169880</td>\n",
       "      <td>0.222112</td>\n",
       "      <td>-0.227179</td>\n",
       "      <td>0.079730</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.090422</td>\n",
       "      <td>0.019275</td>\n",
       "      <td>0.052264</td>\n",
       "      <td>-0.057086</td>\n",
       "      <td>0.088505</td>\n",
       "      <td>-0.072535</td>\n",
       "      <td>0.028086</td>\n",
       "      <td>-0.084547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.070361</td>\n",
       "      <td>-0.143880</td>\n",
       "      <td>0.034461</td>\n",
       "      <td>-0.046620</td>\n",
       "      <td>0.032937</td>\n",
       "      <td>0.073360</td>\n",
       "      <td>-0.013014</td>\n",
       "      <td>0.043715</td>\n",
       "      <td>0.041883</td>\n",
       "      <td>-0.177599</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.061401</td>\n",
       "      <td>0.023023</td>\n",
       "      <td>0.031521</td>\n",
       "      <td>0.008483</td>\n",
       "      <td>-0.076748</td>\n",
       "      <td>-0.053662</td>\n",
       "      <td>0.093354</td>\n",
       "      <td>0.039162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>-0.024271</td>\n",
       "      <td>0.269351</td>\n",
       "      <td>0.030346</td>\n",
       "      <td>-0.108444</td>\n",
       "      <td>0.017094</td>\n",
       "      <td>0.045980</td>\n",
       "      <td>-0.280763</td>\n",
       "      <td>0.086119</td>\n",
       "      <td>-0.510671</td>\n",
       "      <td>0.686397</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.028665</td>\n",
       "      <td>-0.022193</td>\n",
       "      <td>-0.024839</td>\n",
       "      <td>0.032969</td>\n",
       "      <td>-0.068215</td>\n",
       "      <td>-0.058599</td>\n",
       "      <td>0.040410</td>\n",
       "      <td>-0.064413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>0.052785</td>\n",
       "      <td>0.431704</td>\n",
       "      <td>-0.030989</td>\n",
       "      <td>-0.042440</td>\n",
       "      <td>-0.077689</td>\n",
       "      <td>0.068527</td>\n",
       "      <td>-0.273509</td>\n",
       "      <td>0.046213</td>\n",
       "      <td>-0.481031</td>\n",
       "      <td>0.818665</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000708</td>\n",
       "      <td>-0.014953</td>\n",
       "      <td>-0.007984</td>\n",
       "      <td>0.036514</td>\n",
       "      <td>-0.034901</td>\n",
       "      <td>-0.094439</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>-0.019684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>0.315469</td>\n",
       "      <td>0.133743</td>\n",
       "      <td>0.041914</td>\n",
       "      <td>-0.077078</td>\n",
       "      <td>-0.016236</td>\n",
       "      <td>-0.014397</td>\n",
       "      <td>0.399908</td>\n",
       "      <td>-0.372134</td>\n",
       "      <td>0.599839</td>\n",
       "      <td>-0.214021</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.083061</td>\n",
       "      <td>-0.049630</td>\n",
       "      <td>0.042368</td>\n",
       "      <td>-0.011017</td>\n",
       "      <td>-0.008690</td>\n",
       "      <td>-0.014742</td>\n",
       "      <td>-0.058104</td>\n",
       "      <td>0.092201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>-0.292754</td>\n",
       "      <td>0.437095</td>\n",
       "      <td>0.106727</td>\n",
       "      <td>0.073418</td>\n",
       "      <td>-0.045534</td>\n",
       "      <td>0.004260</td>\n",
       "      <td>-0.641362</td>\n",
       "      <td>0.530807</td>\n",
       "      <td>-1.183831</td>\n",
       "      <td>1.476218</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.049516</td>\n",
       "      <td>-0.019234</td>\n",
       "      <td>0.050568</td>\n",
       "      <td>0.080669</td>\n",
       "      <td>0.092384</td>\n",
       "      <td>0.048941</td>\n",
       "      <td>-0.009779</td>\n",
       "      <td>0.052942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>-0.078683</td>\n",
       "      <td>-0.002966</td>\n",
       "      <td>-0.062383</td>\n",
       "      <td>0.063115</td>\n",
       "      <td>0.039862</td>\n",
       "      <td>-0.097507</td>\n",
       "      <td>-0.112859</td>\n",
       "      <td>0.055831</td>\n",
       "      <td>-0.099713</td>\n",
       "      <td>0.131712</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.057992</td>\n",
       "      <td>-0.055418</td>\n",
       "      <td>-0.085551</td>\n",
       "      <td>-0.083351</td>\n",
       "      <td>-0.083109</td>\n",
       "      <td>0.083885</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>-0.038219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0    -0.011059 -0.000688 -0.050495  0.006283 -0.078168  0.060976 -0.020656   \n",
       "1     0.045093  0.357293 -0.052318  0.023984 -0.079437 -0.064630 -0.177694   \n",
       "2    -0.160763 -0.478253 -0.058969  0.097277  0.062118  0.049360  0.098586   \n",
       "3    -0.107175 -0.017778 -0.014382 -0.102119 -0.068903  0.077682 -0.169880   \n",
       "4    -0.070361 -0.143880  0.034461 -0.046620  0.032937  0.073360 -0.013014   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1495 -0.024271  0.269351  0.030346 -0.108444  0.017094  0.045980 -0.280763   \n",
       "1496  0.052785  0.431704 -0.030989 -0.042440 -0.077689  0.068527 -0.273509   \n",
       "1497  0.315469  0.133743  0.041914 -0.077078 -0.016236 -0.014397  0.399908   \n",
       "1498 -0.292754  0.437095  0.106727  0.073418 -0.045534  0.004260 -0.641362   \n",
       "1499 -0.078683 -0.002966 -0.062383  0.063115  0.039862 -0.097507 -0.112859   \n",
       "\n",
       "          7         8         9     ...  9990  9991      9992      9993  \\\n",
       "0     0.064324 -0.039187 -0.050386  ...   1.0   1.0  0.014331  0.041990   \n",
       "1     0.068332 -0.378153  0.679334  ...   1.0   1.0  0.054762 -0.094758   \n",
       "2     0.029956  0.336544 -0.803452  ...  -1.0  -1.0 -0.026777 -0.089011   \n",
       "3     0.222112 -0.227179  0.079730  ...   1.0   1.0 -0.090422  0.019275   \n",
       "4     0.043715  0.041883 -0.177599  ...  -1.0  -1.0  0.061401  0.023023   \n",
       "...        ...       ...       ...  ...   ...   ...       ...       ...   \n",
       "1495  0.086119 -0.510671  0.686397  ...   1.0   1.0 -0.028665 -0.022193   \n",
       "1496  0.046213 -0.481031  0.818665  ...   1.0   1.0 -0.000708 -0.014953   \n",
       "1497 -0.372134  0.599839 -0.214021  ...   1.0   1.0  0.083061 -0.049630   \n",
       "1498  0.530807 -1.183831  1.476218  ...   1.0   1.0 -0.049516 -0.019234   \n",
       "1499  0.055831 -0.099713  0.131712  ...   1.0   1.0 -0.057992 -0.055418   \n",
       "\n",
       "          9994      9995      9996      9997      9998      9999  \n",
       "0     0.062897  0.056659  0.083365 -0.010076  0.019453  0.042835  \n",
       "1    -0.066864 -0.062517  0.042597  0.058450 -0.020841 -0.082203  \n",
       "2     0.047432  0.010113  0.057112  0.007800  0.036619  0.090570  \n",
       "3     0.052264 -0.057086  0.088505 -0.072535  0.028086 -0.084547  \n",
       "4     0.031521  0.008483 -0.076748 -0.053662  0.093354  0.039162  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "1495 -0.024839  0.032969 -0.068215 -0.058599  0.040410 -0.064413  \n",
       "1496 -0.007984  0.036514 -0.034901 -0.094439  0.000687 -0.019684  \n",
       "1497  0.042368 -0.011017 -0.008690 -0.014742 -0.058104  0.092201  \n",
       "1498  0.050568  0.080669  0.092384  0.048941 -0.009779  0.052942  \n",
       "1499 -0.085551 -0.083351 -0.083109  0.083885 -0.000423 -0.038219  \n",
       "\n",
       "[1500 rows x 10000 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9990</th>\n",
       "      <th>9991</th>\n",
       "      <th>9992</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.124533</td>\n",
       "      <td>-0.221424</td>\n",
       "      <td>-0.145173</td>\n",
       "      <td>-0.006669</td>\n",
       "      <td>0.063243</td>\n",
       "      <td>0.045073</td>\n",
       "      <td>-0.002548</td>\n",
       "      <td>0.016390</td>\n",
       "      <td>0.214668</td>\n",
       "      <td>0.073106</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036898</td>\n",
       "      <td>-0.105363</td>\n",
       "      <td>-0.107297</td>\n",
       "      <td>0.008659</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>-0.197312</td>\n",
       "      <td>-0.252470</td>\n",
       "      <td>-0.003405</td>\n",
       "      <td>-0.167395</td>\n",
       "      <td>-0.031711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.143480</td>\n",
       "      <td>-0.236546</td>\n",
       "      <td>-0.147356</td>\n",
       "      <td>-0.005368</td>\n",
       "      <td>0.055398</td>\n",
       "      <td>0.063121</td>\n",
       "      <td>0.008028</td>\n",
       "      <td>0.014522</td>\n",
       "      <td>0.239360</td>\n",
       "      <td>0.095556</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036838</td>\n",
       "      <td>-0.113048</td>\n",
       "      <td>-0.098176</td>\n",
       "      <td>0.025062</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>-0.214869</td>\n",
       "      <td>-0.250675</td>\n",
       "      <td>0.008712</td>\n",
       "      <td>-0.173689</td>\n",
       "      <td>-0.022418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.117838</td>\n",
       "      <td>-0.221106</td>\n",
       "      <td>-0.151592</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.064035</td>\n",
       "      <td>0.058014</td>\n",
       "      <td>0.003266</td>\n",
       "      <td>0.009824</td>\n",
       "      <td>0.227006</td>\n",
       "      <td>0.079053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040504</td>\n",
       "      <td>-0.109898</td>\n",
       "      <td>-0.104012</td>\n",
       "      <td>0.019449</td>\n",
       "      <td>0.020691</td>\n",
       "      <td>-0.194554</td>\n",
       "      <td>-0.238752</td>\n",
       "      <td>-0.006838</td>\n",
       "      <td>-0.152358</td>\n",
       "      <td>-0.031241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.137774</td>\n",
       "      <td>-0.225573</td>\n",
       "      <td>-0.124276</td>\n",
       "      <td>-0.006233</td>\n",
       "      <td>0.059225</td>\n",
       "      <td>0.050032</td>\n",
       "      <td>0.008888</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>0.234306</td>\n",
       "      <td>0.080525</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037641</td>\n",
       "      <td>-0.116229</td>\n",
       "      <td>-0.104199</td>\n",
       "      <td>0.016427</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>-0.189548</td>\n",
       "      <td>-0.249141</td>\n",
       "      <td>0.009519</td>\n",
       "      <td>-0.172564</td>\n",
       "      <td>-0.040607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.121470</td>\n",
       "      <td>-0.217610</td>\n",
       "      <td>-0.154930</td>\n",
       "      <td>-0.004923</td>\n",
       "      <td>0.065214</td>\n",
       "      <td>0.054882</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.015187</td>\n",
       "      <td>0.224034</td>\n",
       "      <td>0.076585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035356</td>\n",
       "      <td>-0.111160</td>\n",
       "      <td>-0.107284</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.014443</td>\n",
       "      <td>-0.205738</td>\n",
       "      <td>-0.238729</td>\n",
       "      <td>-0.009276</td>\n",
       "      <td>-0.154542</td>\n",
       "      <td>-0.026898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>0.119869</td>\n",
       "      <td>-0.230728</td>\n",
       "      <td>-0.137974</td>\n",
       "      <td>-0.001430</td>\n",
       "      <td>0.056711</td>\n",
       "      <td>0.060908</td>\n",
       "      <td>0.005253</td>\n",
       "      <td>0.007011</td>\n",
       "      <td>0.229571</td>\n",
       "      <td>0.084427</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041824</td>\n",
       "      <td>-0.119357</td>\n",
       "      <td>-0.100939</td>\n",
       "      <td>0.015616</td>\n",
       "      <td>0.018071</td>\n",
       "      <td>-0.194115</td>\n",
       "      <td>-0.247263</td>\n",
       "      <td>-0.004955</td>\n",
       "      <td>-0.165272</td>\n",
       "      <td>-0.032881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>0.121795</td>\n",
       "      <td>-0.223846</td>\n",
       "      <td>-0.136487</td>\n",
       "      <td>-0.013410</td>\n",
       "      <td>0.054272</td>\n",
       "      <td>0.056706</td>\n",
       "      <td>0.006234</td>\n",
       "      <td>0.010387</td>\n",
       "      <td>0.225961</td>\n",
       "      <td>0.081148</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031490</td>\n",
       "      <td>-0.123471</td>\n",
       "      <td>-0.105050</td>\n",
       "      <td>0.011833</td>\n",
       "      <td>0.011488</td>\n",
       "      <td>-0.201982</td>\n",
       "      <td>-0.247420</td>\n",
       "      <td>-0.003588</td>\n",
       "      <td>-0.179777</td>\n",
       "      <td>-0.024024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>0.121235</td>\n",
       "      <td>-0.223205</td>\n",
       "      <td>-0.139051</td>\n",
       "      <td>-0.005722</td>\n",
       "      <td>0.056424</td>\n",
       "      <td>0.059435</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>0.009899</td>\n",
       "      <td>0.226712</td>\n",
       "      <td>0.077830</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036718</td>\n",
       "      <td>-0.116827</td>\n",
       "      <td>-0.101447</td>\n",
       "      <td>0.011724</td>\n",
       "      <td>0.010749</td>\n",
       "      <td>-0.202411</td>\n",
       "      <td>-0.250035</td>\n",
       "      <td>-0.006259</td>\n",
       "      <td>-0.168359</td>\n",
       "      <td>-0.024304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>0.126403</td>\n",
       "      <td>-0.216812</td>\n",
       "      <td>-0.154240</td>\n",
       "      <td>-0.006568</td>\n",
       "      <td>0.063909</td>\n",
       "      <td>0.055246</td>\n",
       "      <td>-0.002241</td>\n",
       "      <td>0.013896</td>\n",
       "      <td>0.227570</td>\n",
       "      <td>0.075144</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038583</td>\n",
       "      <td>-0.109065</td>\n",
       "      <td>-0.108984</td>\n",
       "      <td>0.017229</td>\n",
       "      <td>0.010278</td>\n",
       "      <td>-0.207190</td>\n",
       "      <td>-0.244005</td>\n",
       "      <td>-0.009151</td>\n",
       "      <td>-0.158818</td>\n",
       "      <td>-0.022735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>0.122928</td>\n",
       "      <td>-0.219022</td>\n",
       "      <td>-0.151032</td>\n",
       "      <td>-0.004827</td>\n",
       "      <td>0.062587</td>\n",
       "      <td>0.060470</td>\n",
       "      <td>0.011548</td>\n",
       "      <td>0.018048</td>\n",
       "      <td>0.232077</td>\n",
       "      <td>0.081534</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038019</td>\n",
       "      <td>-0.115367</td>\n",
       "      <td>-0.101986</td>\n",
       "      <td>0.024448</td>\n",
       "      <td>0.021036</td>\n",
       "      <td>-0.200604</td>\n",
       "      <td>-0.234022</td>\n",
       "      <td>-0.010822</td>\n",
       "      <td>-0.155221</td>\n",
       "      <td>-0.028157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     0.124533 -0.221424 -0.145173 -0.006669  0.063243  0.045073 -0.002548   \n",
       "1     0.143480 -0.236546 -0.147356 -0.005368  0.055398  0.063121  0.008028   \n",
       "2     0.117838 -0.221106 -0.151592  0.002418  0.064035  0.058014  0.003266   \n",
       "3     0.137774 -0.225573 -0.124276 -0.006233  0.059225  0.050032  0.008888   \n",
       "4     0.121470 -0.217610 -0.154930 -0.004923  0.065214  0.054882  0.001528   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1495  0.119869 -0.230728 -0.137974 -0.001430  0.056711  0.060908  0.005253   \n",
       "1496  0.121795 -0.223846 -0.136487 -0.013410  0.054272  0.056706  0.006234   \n",
       "1497  0.121235 -0.223205 -0.139051 -0.005722  0.056424  0.059435  0.001663   \n",
       "1498  0.126403 -0.216812 -0.154240 -0.006568  0.063909  0.055246 -0.002241   \n",
       "1499  0.122928 -0.219022 -0.151032 -0.004827  0.062587  0.060470  0.011548   \n",
       "\n",
       "          7         8         9     ...      9990      9991      9992  \\\n",
       "0     0.016390  0.214668  0.073106  ... -0.036898 -0.105363 -0.107297   \n",
       "1     0.014522  0.239360  0.095556  ... -0.036838 -0.113048 -0.098176   \n",
       "2     0.009824  0.227006  0.079053  ... -0.040504 -0.109898 -0.104012   \n",
       "3     0.004686  0.234306  0.080525  ... -0.037641 -0.116229 -0.104199   \n",
       "4     0.015187  0.224034  0.076585  ... -0.035356 -0.111160 -0.107284   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1495  0.007011  0.229571  0.084427  ... -0.041824 -0.119357 -0.100939   \n",
       "1496  0.010387  0.225961  0.081148  ... -0.031490 -0.123471 -0.105050   \n",
       "1497  0.009899  0.226712  0.077830  ... -0.036718 -0.116827 -0.101447   \n",
       "1498  0.013896  0.227570  0.075144  ... -0.038583 -0.109065 -0.108984   \n",
       "1499  0.018048  0.232077  0.081534  ... -0.038019 -0.115367 -0.101986   \n",
       "\n",
       "          9993      9994      9995      9996      9997      9998      9999  \n",
       "0     0.008659  0.003418 -0.197312 -0.252470 -0.003405 -0.167395 -0.031711  \n",
       "1     0.025062  0.005454 -0.214869 -0.250675  0.008712 -0.173689 -0.022418  \n",
       "2     0.019449  0.020691 -0.194554 -0.238752 -0.006838 -0.152358 -0.031241  \n",
       "3     0.016427  0.001712 -0.189548 -0.249141  0.009519 -0.172564 -0.040607  \n",
       "4     0.017672  0.014443 -0.205738 -0.238729 -0.009276 -0.154542 -0.026898  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1495  0.015616  0.018071 -0.194115 -0.247263 -0.004955 -0.165272 -0.032881  \n",
       "1496  0.011833  0.011488 -0.201982 -0.247420 -0.003588 -0.179777 -0.024024  \n",
       "1497  0.011724  0.010749 -0.202411 -0.250035 -0.006259 -0.168359 -0.024304  \n",
       "1498  0.017229  0.010278 -0.207190 -0.244005 -0.009151 -0.158818 -0.022735  \n",
       "1499  0.024448  0.021036 -0.200604 -0.234022 -0.010822 -0.155221 -0.028157  \n",
       "\n",
       "[1500 rows x 10000 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first row from the dataframe\n",
    "first_row = reconstructed_df.iloc[0]\n",
    "\n",
    "# Reshape it to (1000, 10)\n",
    "reshaped_array = np.reshape(first_row.values, (1000, 10))\n",
    "\n",
    "# Convert it back to a dataframe\n",
    "reshaped_df = pd.DataFrame(reshaped_array)\n",
    "recon_combined_tensor = torch.tensor(reshaped_df.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/firstenv/lib/python3.11/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Swimmer-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -0.139926\n",
      "1    0.157584\n",
      "Name: 0, dtype: float32\n",
      "Creating window glfw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 13:21:40.365 python[44547:2923270] TSM AdjustCapsLockLEDForKeyTransitionHandling - _ISSetPhysicalKeyboardCapsLockLED Inhibit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.073389\n",
      "1   -0.158975\n",
      "Name: 1, dtype: float32\n",
      "0    0.116139\n",
      "1    0.107686\n",
      "Name: 2, dtype: float32\n",
      "0   -0.114843\n",
      "1   -0.017716\n",
      "Name: 3, dtype: float32\n",
      "0   -0.043089\n",
      "1    0.104847\n",
      "Name: 4, dtype: float32\n",
      "0    0.063043\n",
      "1   -0.052748\n",
      "Name: 5, dtype: float32\n",
      "0    0.219888\n",
      "1    0.087516\n",
      "Name: 6, dtype: float32\n",
      "0   -0.130604\n",
      "1   -0.199224\n",
      "Name: 7, dtype: float32\n",
      "0    0.010136\n",
      "1   -0.156759\n",
      "Name: 8, dtype: float32\n",
      "0   -0.07679\n",
      "1    0.03554\n",
      "Name: 9, dtype: float32\n",
      "0   -0.055452\n",
      "1    0.020870\n",
      "Name: 10, dtype: float32\n",
      "0    0.186233\n",
      "1    0.037425\n",
      "Name: 11, dtype: float32\n",
      "0   -0.144864\n",
      "1   -0.284767\n",
      "Name: 12, dtype: float32\n",
      "0    0.006366\n",
      "1   -0.040872\n",
      "Name: 13, dtype: float32\n",
      "0   -0.205486\n",
      "1    0.178008\n",
      "Name: 14, dtype: float32\n",
      "0   -0.201864\n",
      "1   -0.031993\n",
      "Name: 15, dtype: float32\n",
      "0   -0.264236\n",
      "1   -0.218248\n",
      "Name: 16, dtype: float32\n",
      "0    0.025881\n",
      "1   -0.167981\n",
      "Name: 17, dtype: float32\n",
      "0   -0.029662\n",
      "1    0.221602\n",
      "Name: 18, dtype: float32\n",
      "0   -0.182867\n",
      "1   -0.103092\n",
      "Name: 19, dtype: float32\n",
      "0   -0.112464\n",
      "1    0.234400\n",
      "Name: 20, dtype: float32\n",
      "0    0.048177\n",
      "1    0.261739\n",
      "Name: 21, dtype: float32\n",
      "0   -0.221350\n",
      "1    0.045695\n",
      "Name: 22, dtype: float32\n",
      "0   -0.161030\n",
      "1    0.205413\n",
      "Name: 23, dtype: float32\n",
      "0    0.049421\n",
      "1   -0.052769\n",
      "Name: 24, dtype: float32\n",
      "0    0.141947\n",
      "1   -0.163813\n",
      "Name: 25, dtype: float32\n",
      "0    0.131522\n",
      "1    0.056941\n",
      "Name: 26, dtype: float32\n",
      "0    0.156686\n",
      "1   -0.342054\n",
      "Name: 27, dtype: float32\n",
      "0   -0.103833\n",
      "1   -0.268281\n",
      "Name: 28, dtype: float32\n",
      "0    0.198624\n",
      "1   -0.193563\n",
      "Name: 29, dtype: float32\n",
      "0   -0.031347\n",
      "1   -0.011492\n",
      "Name: 30, dtype: float32\n",
      "0   -0.106881\n",
      "1   -0.202843\n",
      "Name: 31, dtype: float32\n",
      "0    0.094878\n",
      "1   -0.303446\n",
      "Name: 32, dtype: float32\n",
      "0    0.230221\n",
      "1   -0.137614\n",
      "Name: 33, dtype: float32\n",
      "0   -0.221525\n",
      "1    0.177959\n",
      "Name: 34, dtype: float32\n",
      "0    0.352117\n",
      "1    0.133101\n",
      "Name: 35, dtype: float32\n",
      "0    0.138195\n",
      "1    0.034364\n",
      "Name: 36, dtype: float32\n",
      "0   -0.062143\n",
      "1    0.007863\n",
      "Name: 37, dtype: float32\n",
      "0    0.219358\n",
      "1    0.123215\n",
      "Name: 38, dtype: float32\n",
      "0    0.049722\n",
      "1    0.095333\n",
      "Name: 39, dtype: float32\n",
      "0    0.126416\n",
      "1   -0.192249\n",
      "Name: 40, dtype: float32\n",
      "0    0.060357\n",
      "1   -0.232144\n",
      "Name: 41, dtype: float32\n",
      "0   -0.040395\n",
      "1   -0.338716\n",
      "Name: 42, dtype: float32\n",
      "0    0.195540\n",
      "1   -0.236515\n",
      "Name: 43, dtype: float32\n",
      "0   -0.139073\n",
      "1   -0.047359\n",
      "Name: 44, dtype: float32\n",
      "0   -0.142431\n",
      "1    0.099266\n",
      "Name: 45, dtype: float32\n",
      "0    0.067819\n",
      "1   -0.163791\n",
      "Name: 46, dtype: float32\n",
      "0   -0.267317\n",
      "1    0.150573\n",
      "Name: 47, dtype: float32\n",
      "0   -0.203397\n",
      "1    0.038247\n",
      "Name: 48, dtype: float32\n",
      "0    0.015370\n",
      "1   -0.306025\n",
      "Name: 49, dtype: float32\n",
      "0   -0.034039\n",
      "1   -0.201009\n",
      "Name: 50, dtype: float32\n",
      "0    0.148633\n",
      "1   -0.284088\n",
      "Name: 51, dtype: float32\n",
      "0   -0.166006\n",
      "1   -0.071162\n",
      "Name: 52, dtype: float32\n",
      "0    0.010471\n",
      "1    0.202508\n",
      "Name: 53, dtype: float32\n",
      "0    0.106469\n",
      "1   -0.040526\n",
      "Name: 54, dtype: float32\n",
      "0    0.105194\n",
      "1   -0.154886\n",
      "Name: 55, dtype: float32\n",
      "0   -0.288241\n",
      "1    0.230703\n",
      "Name: 56, dtype: float32\n",
      "0   -0.027264\n",
      "1    0.196237\n",
      "Name: 57, dtype: float32\n",
      "0    0.109205\n",
      "1   -0.083717\n",
      "Name: 58, dtype: float32\n",
      "0   -0.083159\n",
      "1   -0.054008\n",
      "Name: 59, dtype: float32\n",
      "0   -0.039136\n",
      "1   -0.103068\n",
      "Name: 60, dtype: float32\n",
      "0   -0.214544\n",
      "1    0.210843\n",
      "Name: 61, dtype: float32\n",
      "0    0.144477\n",
      "1    0.256160\n",
      "Name: 62, dtype: float32\n",
      "0    0.324265\n",
      "1    0.222987\n",
      "Name: 63, dtype: float32\n",
      "0   -0.016786\n",
      "1    0.014369\n",
      "Name: 64, dtype: float32\n",
      "0    0.079667\n",
      "1   -0.251728\n",
      "Name: 65, dtype: float32\n",
      "0    0.138008\n",
      "1    0.164873\n",
      "Name: 66, dtype: float32\n",
      "0   -0.083776\n",
      "1    0.161418\n",
      "Name: 67, dtype: float32\n",
      "0    0.199551\n",
      "1   -0.015483\n",
      "Name: 68, dtype: float32\n",
      "0   -0.024103\n",
      "1    0.176124\n",
      "Name: 69, dtype: float32\n",
      "0   -0.168421\n",
      "1    0.254656\n",
      "Name: 70, dtype: float32\n",
      "0    0.058565\n",
      "1    0.240024\n",
      "Name: 71, dtype: float32\n",
      "0    0.026116\n",
      "1   -0.232451\n",
      "Name: 72, dtype: float32\n",
      "0    0.080883\n",
      "1   -0.171363\n",
      "Name: 73, dtype: float32\n",
      "0   -0.078258\n",
      "1   -0.233103\n",
      "Name: 74, dtype: float32\n",
      "0   -0.005681\n",
      "1    0.025008\n",
      "Name: 75, dtype: float32\n",
      "0    0.249984\n",
      "1   -0.359160\n",
      "Name: 76, dtype: float32\n",
      "0   -0.055560\n",
      "1   -0.158267\n",
      "Name: 77, dtype: float32\n",
      "0    0.301619\n",
      "1   -0.185667\n",
      "Name: 78, dtype: float32\n",
      "0   -0.189818\n",
      "1   -0.245241\n",
      "Name: 79, dtype: float32\n",
      "0    0.240619\n",
      "1   -0.087645\n",
      "Name: 80, dtype: float32\n",
      "0    0.191256\n",
      "1    0.009645\n",
      "Name: 81, dtype: float32\n",
      "0    0.171693\n",
      "1    0.146717\n",
      "Name: 82, dtype: float32\n",
      "0    0.161663\n",
      "1   -0.016897\n",
      "Name: 83, dtype: float32\n",
      "0   -0.124715\n",
      "1    0.054573\n",
      "Name: 84, dtype: float32\n",
      "0   -0.278997\n",
      "1   -0.091884\n",
      "Name: 85, dtype: float32\n",
      "0   -0.170351\n",
      "1   -0.170131\n",
      "Name: 86, dtype: float32\n",
      "0   -0.201724\n",
      "1    0.198895\n",
      "Name: 87, dtype: float32\n",
      "0   -0.157955\n",
      "1    0.248568\n",
      "Name: 88, dtype: float32\n",
      "0   -0.156287\n",
      "1    0.122933\n",
      "Name: 89, dtype: float32\n",
      "0    0.265778\n",
      "1    0.222656\n",
      "Name: 90, dtype: float32\n",
      "0    0.222014\n",
      "1    0.100686\n",
      "Name: 91, dtype: float32\n",
      "0    0.137023\n",
      "1    0.102517\n",
      "Name: 92, dtype: float32\n",
      "0    0.274772\n",
      "1   -0.149991\n",
      "Name: 93, dtype: float32\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/firstenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "replay(reshaped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first row from the dataframe\n",
    "first_row = original_df.iloc[0]\n",
    "\n",
    "# Reshape it to (1000, 10)\n",
    "reshaped_array = np.reshape(first_row.values, (1000, 10))\n",
    "\n",
    "# Convert it back to a dataframe\n",
    "reshaped_df = pd.DataFrame(reshaped_array)\n",
    "ori_combined_tensor = torch.tensor(reshaped_df.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay(ori_combined_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1198, -0.1550,  0.4563,  ..., -0.1441,  0.6658, -0.5765],\n",
       "        [ 1.1892, -0.3707, -0.1260,  ..., -1.6430,  2.8639, -2.4233],\n",
       "        [ 0.8910,  1.0391, -0.2096,  ..., -1.5129,  2.9911, -1.2082],\n",
       "        ...,\n",
       "        [-1.3420, -1.0858, -0.3244,  ...,  2.4165, -3.8592, -0.2681],\n",
       "        [-1.0497, -0.8429, -0.3485,  ...,  2.6913, -3.2114, -0.9843],\n",
       "        [-0.7667, -0.7755, -0.0154,  ...,  0.2855, -0.1879,  0.0838]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_combined_tensor-ori_combined_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(159.2569)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the tensors\n",
    "flattened_ori = ori_combined_tensor.flatten()\n",
    "flattened_recon = recon_combined_tensor.flatten()\n",
    "\n",
    "# Calculate the Euclidean distance\n",
    "euclidean_distance = torch.norm(flattened_ori - flattened_recon)\n",
    "\n",
    "\n",
    "print(euclidean_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class VRNN(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, z_dim, n_layers, dropout=0.):\n",
    "        super(VRNN, self).__init__()\n",
    "\n",
    "        self.x_dim = x_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.encoder = nn.LSTM(x_dim + h_dim, z_dim, n_layers, dropout=dropout)\n",
    "        self.decoder = nn.LSTM(z_dim + h_dim, x_dim, n_layers, dropout=dropout)\n",
    "        self.hidden_to_hidden = nn.Linear(z_dim, h_dim)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.zeros(self.n_layers, x.size(1), self.h_dim).to(x.device)\n",
    "\n",
    "        for t in range(x.size(0)):\n",
    "            _, h = self.encoder(torch.cat([x[t], h[-1]], -1).unsqueeze(0), h)\n",
    "            mu, log_var = h.chunk(2, -1)\n",
    "            z = self.reparameterize(mu, log_var)\n",
    "            _, h = self.decoder(torch.cat([z, h[-1]], -1).unsqueeze(0), h)\n",
    "            h = self.hidden_to_hidden(h)\n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For batched 3-D input, hx and cx should also be 3-D but got (2-D, 2-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m batch_data \u001b[38;5;241m=\u001b[39m batch  \u001b[38;5;66;03m# get the data from the batch\u001b[39;00m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 29\u001b[0m h \u001b[38;5;241m=\u001b[39m model(batch_data)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Loss: reconstruction loss + KL divergence\u001b[39;00m\n\u001b[1;32m     32\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m loss_fn(h, batch_data)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/firstenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[23], line 26\u001b[0m, in \u001b[0;36mVRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_dim)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n\u001b[0;32m---> 26\u001b[0m     _, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(torch\u001b[38;5;241m.\u001b[39mcat([x[t], h[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), h)\n\u001b[1;32m     27\u001b[0m     mu, log_var \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, log_var)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/firstenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/firstenv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:798\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m    796\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor batched 3-D input, hx and cx should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    797\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso be 3-D but got (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D) tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 798\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: For batched 3-D input, hx and cx should also be 3-D but got (2-D, 2-D) tensors"
     ]
    }
   ],
   "source": [
    "# Define the data loader\n",
    "batch_size = 512  # adjust as necessary\n",
    "seq_length = 1000  # sequence length\n",
    "\n",
    "# Split data into train, validation, and test\n",
    "train_size = int(0.7 * len(dataset))  # 70% for training\n",
    "valid_size = int(0.15 * len(dataset))  # 15% for validation\n",
    "test_size = len(dataset) - train_size - valid_size  # 15% for testing\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "model = VRNN(x_dim=10, h_dim=128, z_dim=20, n_layers=2, dropout=0.2)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)  # Make sure you're using the correct optimizer\n",
    "loss_fn = nn.MSELoss()  # And the correct loss function\n",
    "\n",
    "# Training\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    for i, batch in enumerate(train_loader):  # using train_loader instead of dataloader\n",
    "        batch_data = batch  # get the data from the batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        h = model(batch_data)\n",
    "\n",
    "        # Loss: reconstruction loss + KL divergence\n",
    "        recon_loss = loss_fn(h, batch_data)\n",
    "        loss = recon_loss  # modify this line to include KL divergence\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'====> Epoch: {epoch} Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      2\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    batch_data = batch\n",
    "    print(batch_data.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# render with action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1000, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Convert combined_arr to PyTorch Tensor\n",
    "# combined_tensor = torch.from_numpy(combined_arr)\n",
    "\n",
    "# Print the shape of combined_tensor\n",
    "print(combined_tensor.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import gym\n",
    "\n",
    "\n",
    "def replay(combined_data):\n",
    "\n",
    "    action_sp = combined_data.iloc[:, :2]\n",
    "    obs_sp = combined_data.iloc[:, 2:]\n",
    "\n",
    "    env = gym.make('Swimmer-v3', render_mode = 'human')\n",
    "\n",
    "    # Iterate through the rows\n",
    "    for i in range(len(action_sp)):\n",
    "        # Get the i-th row\n",
    "        action = action_sp.iloc[i]\n",
    "        observation = obs_sp.iloc[i]\n",
    "        print(action)\n",
    "\n",
    "        # If this is the first iteration, set the environment state to the given observation\n",
    "        # Note: This assumes that the observation you've stored is the entire state that can be set with `env.reset()`\n",
    "        # If this is not the case, you cannot simply set the environment state to the observation\n",
    "        if i == 0:\n",
    "            env.reset()  # We ignore the initial observation returned by `reset`\n",
    "\n",
    "        # Apply the action\n",
    "        next_observation, reward, done, trunc, info = env.step(action)\n",
    "        # Render the environment\n",
    "        env.render()\n",
    "        # If you want to slow down each step for viewing, you can use time.sleep\n",
    "        # time.sleep(0.01)\n",
    "\n",
    "    # Close the environment\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
