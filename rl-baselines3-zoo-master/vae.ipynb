{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import shape,math\n",
    "from tensorflow.keras import Input,layers,Model\n",
    "from tensorflow.keras.losses import mse,binary_crossentropy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import torch; torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# output_dir = os.path.join(\".\", \"output\")  # Path to the output directory\n",
    "# subdirs = [f.path for f in os.scandir(output_dir) if f.is_dir()]\n",
    "# subdirs.sort()\n",
    "\n",
    "# # Create an empty 3D array to store the combined data\n",
    "# combined_arr = np.empty((len(subdirs), 999, 10))\n",
    "\n",
    "# # Loop through each subdirectory and load the CSV files\n",
    "# for i, subdir in enumerate(subdirs):\n",
    "#     action_filename = os.path.join(subdir, \"action.csv\")\n",
    "#     obs_filename = os.path.join(subdir, \"obs.csv\")\n",
    "\n",
    "#     # Load the action and obs CSV files\n",
    "#     action_df = pd.read_csv(action_filename)\n",
    "#     obs_df = pd.read_csv(obs_filename)\n",
    "\n",
    "#     # Concatenate the DataFrames horizontally\n",
    "#     combined_data = pd.concat([action_df, obs_df], axis=1)\n",
    "\n",
    "#     # Convert combined_data to a 3D array and assign it to combined_arr\n",
    "#     combined_arr[i] = np.reshape(combined_data.values, (999, 10))\n",
    "\n",
    "# # Print the shape of combined_arr\n",
    "# print(combined_arr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattened_arr = combined_arr.reshape(10000, 9990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattened_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename dimensions to 'Seed' and 'Data'\n",
    "# flattened_tensor.names = (\"Seed\", \"Data\")\n",
    "\n",
    "# # Now flattened_tensor will have dimensions named 'Seed' and 'Data'\n",
    "# print(flattened_tensor.shape)  # Output: torch.Size([10000, 9990])\n",
    "# print(flattened_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(flattened_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "output_dir = os.path.join(\".\", \"output_bw\")  # Path to the output directory\n",
    "subdirs = [f.path for f in os.scandir(output_dir) if f.is_dir()]\n",
    "subdirs.sort()\n",
    "\n",
    "# Create an empty 3D array to store the combined data\n",
    "combined_arr = np.empty((len(subdirs), 1000, 10))\n",
    "\n",
    "# Loop through each subdirectory and load the CSV files\n",
    "for i, subdir in enumerate(subdirs):\n",
    "    action_filename = os.path.join(subdir, \"action.csv\")\n",
    "    obs_filename = os.path.join(subdir, \"obs.csv\")\n",
    "\n",
    "    # Load the action and obs CSV files\n",
    "    action_df = pd.read_csv(action_filename,  header=None)\n",
    "    obs_df = pd.read_csv(obs_filename,  header=None)\n",
    "\n",
    "    # Concatenate the DataFrames horizontally\n",
    "    combined_data = pd.concat([action_df, obs_df], axis=1)\n",
    "\n",
    "    # Convert combined_data to a 3D array and assign it to combined_arr\n",
    "    combined_arr[i-1] = np.reshape(combined_data.values, (1000, 10))\n",
    "\n",
    "# Print the shape of combined_arr\n",
    "print(combined_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = np.array(combined_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "## 정규화하기\n",
    "\n",
    "# Flatten the combined_arr to 2D\n",
    "flattened_arr = combined_arr.reshape(-1, combined_arr.shape[-1])\n",
    "\n",
    "# Create a scaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler and transform the data\n",
    "normalized_arr = scaler.fit_transform(flattened_arr)\n",
    "\n",
    "# Reshape the normalized data back to the original shape\n",
    "normalized_arr = normalized_arr.reshape(combined_arr.shape)\n",
    "\n",
    "# Print the shape of normalized_arr\n",
    "print(normalized_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = np.array(combined_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 정규화 다시 되돌리기\n",
    "\n",
    "# Flatten the normalized_arr to 2D\n",
    "flattened_normalized_arr = normalized_arr.reshape(-1, normalized_arr.shape[-1])\n",
    "\n",
    "# Use the inverse_transform method to transform the data back to its original state\n",
    "original_arr = scaler.inverse_transform(flattened_normalized_arr)\n",
    "\n",
    "# Reshape the original data back to the original shape\n",
    "original_arr = original_arr.reshape(combined_arr.shape)\n",
    "\n",
    "# Print the shape of original_arr\n",
    "print(original_arr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "output_dir = os.path.join(\".\", \"output_front\")  # Path to the output directory\n",
    "subdirs = [f.path for f in os.scandir(output_dir) if f.is_dir()]\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Create a tqdm progress bar\n",
    "pbar = tqdm(subdirs, desc=\"Processing subdirectories\", unit=\"subdir\")\n",
    "\n",
    "for subdir in pbar:\n",
    "    action_filename = os.path.join(subdir, \"action.csv\")\n",
    "    obs_filename = os.path.join(subdir, \"obs.csv\")\n",
    "    action_df = pd.read_csv(action_filename)\n",
    "    obs_df = pd.read_csv(obs_filename)\n",
    "    combined_data = pd.concat([action_df, obs_df], axis=1)\n",
    "    if combined_data.shape == (999,10):\n",
    "        combined_df = pd.concat([combined_df, combined_data], ignore_index=True)\n",
    "    else:\n",
    "        print(\"fail : \", subdir)\n",
    "\n",
    "    # Update the progress bar\n",
    "    pbar.set_postfix({\"Processed subdirectories\": subdir})\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "combined_tensor = torch.from_numpy(normalized_arr)\n",
    "print(combined_tensor.shape)\n",
    "flattened_tensor = combined_tensor.permute(2, 0, 1).flatten(start_dim=1)\n",
    "\n",
    "print(flattened_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# combined_df = np.array(combined_arr)\n",
    "# combined_tensor = torch.from_numpy(combined_df)\n",
    "# combined_tensor = combined_tensor.permute(2, 0, 1).flatten(start_dim=1)\n",
    "\n",
    "# print(combined_tensor.shape[0])\n",
    "# print(combined_tensor.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE code -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# Assuming combined_tensor is your data\n",
    "# Convert the data to float32\n",
    "dataset = TensorDataset(combined_tensor.float())\n",
    "\n",
    "# Define the data loader\n",
    "batch_size = 256  # adjust as necessary\n",
    "\n",
    "# Split data into train, validation, and test\n",
    "train_size = int(0.7 * len(dataset))  # 70% for training\n",
    "valid_size = int(0.15 * len(dataset))  # 15% for validation\n",
    "test_size = len(dataset) - train_size - valid_size  # 15% for testing\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Hyperparameters\n",
    "input_shape = combined_tensor.shape[1] * combined_tensor.shape[2]  # modify this to match your data\n",
    "hidden_dim1 = 512  # modify as needed\n",
    "hidden_dim2 = 256  # modify as needed\n",
    "hidden_dim3 = 126  # modify as needed\n",
    "latent_dim = 2  # modify as needed\n",
    "lr = 1e-3  # learning rate\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_shape, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim3, latent_dim * 2)  # mean and variance\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim3, input_shape),\n",
    "#             nn.Sigmoid()      ## 시그모이드를 뺴야됨. 왜냐면 애초데이터가 0과 1사이가 아니기때문\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = h.chunk(2, dim=1)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, mu, log_var\n",
    "    \n",
    "# Model, optimizer, and loss function\n",
    "model = VAE(input_shape, hidden_dim1, latent_dim)\n",
    "\n",
    "optimizer = optim.NAdam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss() #MSE로 바꿔보자.\n",
    "\n",
    "def train(epoch, model, optimizer, loss_fn, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kl_div = 0\n",
    "    for i, batch in enumerate(train_loader):  # using train_loader instead of dataloader\n",
    "        batch_data = batch[0]  # get the data from the batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Flatten the data\n",
    "        batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "\n",
    "        reconstructed_batch, mu, log_var = model(batch_data)\n",
    "\n",
    "        # Loss: reconstruction loss + KL divergence\n",
    "        recon_loss = loss_fn(reconstructed_batch, batch_data)\n",
    "        kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        loss = recon_loss + kl_divergence\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_kl_div += kl_divergence.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_div = total_kl_div / len(train_loader.dataset)\n",
    "    print(f'====> Epoch: {epoch} Average loss: {avg_loss}, Recon Loss: {avg_recon_loss}, KL Div: {avg_kl_div}')\n",
    "\n",
    "    return avg_loss, avg_recon_loss, avg_kl_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lists to store losses for each epoch\n",
    "avg_losses = []\n",
    "avg_recon_losses = []\n",
    "avg_kl_divs = []\n",
    "\n",
    "# Training\n",
    "n_epochs = 50  # modify as needed\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    avg_loss, avg_recon_loss, avg_kl_div = train(epoch, model, optimizer, loss_fn, train_loader)\n",
    "    avg_losses.append(avg_loss)\n",
    "    avg_recon_losses.append(avg_recon_loss)\n",
    "    avg_kl_divs.append(avg_kl_div)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(avg_losses, label='Average Loss')\n",
    "plt.plot(avg_recon_losses, label='Reconstruction Loss')\n",
    "plt.plot(avg_kl_divs, label='KL Divergence')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE code -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_shape, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim3, latent_dim * 2)  # mean and variance\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = h.chunk(2, dim=1)\n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_shape):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim3, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, output_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_shape, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim1, hidden_dim2, hidden_dim3, input_shape)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, mu, log_var\n",
    "\n",
    "    \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# Assuming combined_tensor is your data\n",
    "# Convert the data to float32\n",
    "dataset = TensorDataset(combined_tensor.float())\n",
    "\n",
    "# Define the data loader\n",
    "batch_size = 512  # adjust as necessary\n",
    "\n",
    "# Split data into train, validation, and test\n",
    "train_size = int(0.7 * len(dataset))  # 70% for training\n",
    "valid_size = int(0.15 * len(dataset))  # 15% for validation\n",
    "test_size = len(dataset) - train_size - valid_size  # 15% for testing\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Hyperparameters\n",
    "input_shape = combined_tensor.shape[1] * combined_tensor.shape[2]  # modify this to match your data\n",
    "hidden_dim1 = 512  # modify as needed\n",
    "hidden_dim2 = 256  # modify as needed\n",
    "hidden_dim3 = 24  # modify as needed\n",
    "latent_dim = 2  # modify as needed\n",
    "lr = 5e-5  # learning rate\n",
    "n_epochs = 400  # modify as needed\n",
    "beta = 0.2\n",
    "    \n",
    "# Model, optimizer, and loss function\n",
    "model = VAE(input_shape, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim)\n",
    "\n",
    "optimizer = optim.RAdam(model.parameters(), lr=lr)  # Make sure you're using the correct optimizer\n",
    "loss_fn = nn.MSELoss()  # And the correct loss function\n",
    "\n",
    "\n",
    "def train(epoch, model, optimizer, loss_fn, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kl_div = 0\n",
    "    for i, batch in enumerate(train_loader):  # using train_loader instead of dataloader\n",
    "        batch_data = batch[0]  # get the data from the batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Flatten the data\n",
    "        batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "\n",
    "        reconstructed_batch, mu, log_var = model(batch_data)\n",
    "\n",
    "        # Loss: reconstruction loss + KL divergence\n",
    "        recon_loss = loss_fn(reconstructed_batch, batch_data)\n",
    "        kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        loss = recon_loss + beta*kl_divergence\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_kl_div += kl_divergence.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_div = total_kl_div / len(train_loader.dataset)\n",
    "    print(f'====> Epoch: {epoch} Average loss: {avg_loss}, Recon Loss: {avg_recon_loss}, KL Div: {avg_kl_div}')\n",
    "\n",
    "    return avg_loss, avg_recon_loss, avg_kl_div\n",
    "\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lists to store losses for each epoch\n",
    "avg_losses = []\n",
    "avg_recon_losses = []\n",
    "avg_kl_divs = []\n",
    "\n",
    "# Training\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    avg_loss, avg_recon_loss, avg_kl_div = train(epoch, model, optimizer, loss_fn, train_loader)\n",
    "    avg_losses.append(avg_loss)\n",
    "    avg_recon_losses.append(avg_recon_loss)\n",
    "    avg_kl_divs.append(avg_kl_div)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(avg_losses, label='Average Loss')\n",
    "plt.plot(avg_recon_losses, label='Reconstruction Loss')\n",
    "plt.plot(avg_kl_divs, label='KL Divergence')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kl_div = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            batch_data = batch[0]  # get the data from the batch\n",
    "\n",
    "            # Flatten the data\n",
    "            batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "\n",
    "            reconstructed_batch, mu, log_var = model(batch_data)\n",
    "\n",
    "            # Loss: reconstruction loss + KL divergence\n",
    "            recon_loss = loss_fn(reconstructed_batch, batch_data)\n",
    "            kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "            loss += recon_loss + kl_divergence\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_kl_div += kl_divergence.item()\n",
    "\n",
    "    avg_loss = loss / len(dataloader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(dataloader.dataset)\n",
    "    avg_kl_div = total_kl_div / len(dataloader.dataset)\n",
    "\n",
    "    return avg_loss, avg_recon_loss, avg_kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, loss_fn, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE code -3 [10,1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_shape, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim3, latent_dim * 2)  # mean and variance\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = h.chunk(2, dim=1)\n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_shape):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim3, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, output_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_shape, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim1, hidden_dim2, hidden_dim3, input_shape)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, mu, log_var\n",
    "\n",
    "    \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import torch\n",
    "\n",
    "combined_df = np.array(combined_arr)\n",
    "combined_tensor = torch.from_numpy(combined_df)\n",
    "combined_tensor = combined_tensor.permute(2, 0, 1).flatten(start_dim=1)\n",
    "\n",
    "# Assuming combined_tensor is your data\n",
    "# Convert the data to float32\n",
    "dataset = TensorDataset(combined_tensor.float())\n",
    "\n",
    "# Define the data loader\n",
    "batch_size = 128  # adjust as necessary\n",
    "\n",
    "# Split data into train, validation, and test\n",
    "train_size = int(0.7 * len(dataset))  # 70% for training\n",
    "valid_size = int(0.15 * len(dataset))  # 15% for validation\n",
    "test_size = len(dataset) - train_size - valid_size  # 15% for testing\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "# input_shape = combined_tensor.shape[0] * combined_tensor.shape[1]\n",
    "input_shape = combined_tensor.shape[1]\n",
    "hidden_dim1 = 1024  # modify as needed\n",
    "hidden_dim2 = 128  # modify as needed\n",
    "hidden_dim3 = 12  # modify as needed\n",
    "latent_dim = 2  # modify as needed\n",
    "lr = 5e-4  # learning rate\n",
    "n_epochs = 300  # modify as needed\n",
    "beta = 0.2\n",
    "\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "model = VAE(input_shape, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim)\n",
    "\n",
    "optimizer = optim.RAdam(model.parameters(), lr=lr)  # Make sure you're using the correct optimizer\n",
    "loss_fn = nn.MSELoss()  # And the correct loss function\n",
    "\n",
    "\n",
    "def train(epoch, model, optimizer, loss_fn, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kl_div = 0\n",
    "    for i, batch in enumerate(train_loader):  # using train_loader instead of dataloader\n",
    "        batch_data = batch[0]  # get the data from the batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Flatten the data\n",
    "        batch_data = batch_data.view(-1, input_shape)\n",
    "\n",
    "        reconstructed_batch, mu, log_var = model(batch_data)\n",
    "\n",
    "        # Loss: reconstruction loss + KL divergence\n",
    "        recon_loss = loss_fn(reconstructed_batch, batch_data)\n",
    "        kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        loss = recon_loss + beta*kl_divergence\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_kl_div += kl_divergence.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_div = total_kl_div / len(train_loader.dataset)\n",
    "    print(f'====> Epoch: {epoch} Average loss: {avg_loss}, Recon Loss: {avg_recon_loss}, KL Div: {avg_kl_div}')\n",
    "\n",
    "    return avg_loss, avg_recon_loss, avg_kl_div\n",
    "\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lists to store losses for each epoch\n",
    "avg_losses = []\n",
    "avg_recon_losses = []\n",
    "avg_kl_divs = []\n",
    "\n",
    "# Training\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    avg_loss, avg_recon_loss, avg_kl_div = train(epoch, model, optimizer, loss_fn, train_loader)\n",
    "    avg_losses.append(avg_loss)\n",
    "    avg_recon_losses.append(avg_recon_loss)\n",
    "    avg_kl_divs.append(avg_kl_div)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(avg_losses, label='Average Loss')\n",
    "plt.plot(avg_recon_losses, label='Reconstruction Loss')\n",
    "plt.plot(avg_kl_divs, label='KL Divergence')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# latent space plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get latent space representations for all data points\n",
    "latent_vectors = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch_data = batch[0]\n",
    "        batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "        mu, log_var = model.encoder(batch_data)\n",
    "        z = model.reparameterize(mu, log_var)\n",
    "        latent_vectors.append(z)\n",
    "\n",
    "dance_latent_vectors = torch.cat(latent_vectors, dim=0).numpy()\n",
    "\n",
    "# Plot the data in 2D latent space\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(dance_latent_vectors[:, 0], dance_latent_vectors[:, 1], alpha=0.5)\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('2D Latent Space Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have right_latent_vectors and front_latent_vectors numpy arrays\n",
    "\n",
    "# Plot the data in 2D latent space with different colors\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(right_latent_vectors[:, 0], right_latent_vectors[:, 1], alpha=0.5, c='blue', label='Right Data')\n",
    "plt.scatter(front_latent_vectors[:, 0], front_latent_vectors[:, 1], alpha=0.5, c='red', label='Front Data')\n",
    "plt.scatter(bw_latent_vectors[:, 0], bw_latent_vectors[:, 1], alpha=0.5, c='black', label='Front Data')\n",
    "plt.scatter(dance_latent_vectors[:, 0], dance_latent_vectors[:, 1], alpha=0.5, c='green', label='Dance Data')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('2D Latent Space Visualization')\n",
    "plt.legend()  # Show the legend with label information\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plots  - front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "\n",
    "import umap\n",
    "\n",
    "def visualize_latent_space_umap(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_data = batch[0]\n",
    "            mu, _ = model.encoder(batch_data.view(batch_data.size(0), -1))\n",
    "            latents.append(mu)\n",
    "    latents = torch.cat(latents).numpy()\n",
    "    reducer = umap.UMAP()\n",
    "    embedding = reducer.fit_transform(latents)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, s=3)\n",
    "    plt.title(\"UMAP\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space_umap(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_latent_space_pca(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_data = batch[0]\n",
    "            mu, _ = model.encoder(batch_data.view(batch_data.size(0), -1))\n",
    "            latents.append(mu)\n",
    "    latents = torch.cat(latents).numpy()\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(latents)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(principalComponents[:, 0], principalComponents[:, 1], alpha=0.5, s=3)\n",
    "    plt.title(\"PCA\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space_pca(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-sne\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_latent_space_tsne(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_data = batch[0]\n",
    "            mu, _ = model.encoder(batch_data.view(batch_data.size(0), -1))\n",
    "            latents.append(mu)\n",
    "    latents = torch.cat(latents).numpy()\n",
    "    tsne = TSNE(n_components=2)\n",
    "    tsne_results = tsne.fit_transform(latents)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(tsne_results[:, 0], tsne_results[:, 1], alpha=0.5, s=3)\n",
    "    plt.title(\"t-SNE\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space_tsne(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_latent_space_heatmap(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_data = batch[0]\n",
    "            mu, _ = model.encoder(batch_data.view(batch_data.size(0), -1))\n",
    "            latents.append(mu)\n",
    "    latents = torch.cat(latents).numpy()\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.heatmap(latents)\n",
    "    plt.title(\"Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space_heatmap(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plots  - bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "\n",
    "import umap\n",
    "\n",
    "def visualize_latent_space_umap(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_data = batch[0]\n",
    "            mu, _ = model.encoder(batch_data.view(batch_data.size(0), -1))\n",
    "            latents.append(mu)\n",
    "    latents = torch.cat(latents).numpy()\n",
    "    reducer = umap.UMAP()\n",
    "    embedding = reducer.fit_transform(latents)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, s=3)\n",
    "    plt.title(\"UMAP\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space_umap(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_latent_space_pca(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_data = batch[0]\n",
    "            mu, _ = model.encoder(batch_data.view(batch_data.size(0), -1))\n",
    "            latents.append(mu)\n",
    "    latents = torch.cat(latents).numpy()\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(latents)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(principalComponents[:, 0], principalComponents[:, 1], alpha=0.5, s=3)\n",
    "    plt.title(\"PCA\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space_pca(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-sne\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_latent_space_tsne(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_data = batch[0]\n",
    "            mu, _ = model.encoder(batch_data.view(batch_data.size(0), -1))\n",
    "            latents.append(mu)\n",
    "    latents = torch.cat(latents).numpy()\n",
    "    tsne = TSNE(n_components=2)\n",
    "    tsne_results = tsne.fit_transform(latents)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(tsne_results[:, 0], tsne_results[:, 1], alpha=0.5, s=3)\n",
    "    plt.title(\"t-SNE\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space_tsne(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_latent_space_heatmap(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_data = batch[0]\n",
    "            mu, _ = model.encoder(batch_data.view(batch_data.size(0), -1))\n",
    "            latents.append(mu)\n",
    "    latents = torch.cat(latents).numpy()\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.heatmap(latents)\n",
    "    plt.title(\"Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space_heatmap(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plots  - dance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "\n",
    "import umap\n",
    "\n",
    "def visualize_latent_space_umap(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_data = batch[0]\n",
    "            mu, _ = model.encoder(batch_data.view(batch_data.size(0), -1))\n",
    "            latents.append(mu)\n",
    "    latents = torch.cat(latents).numpy()\n",
    "    reducer = umap.UMAP()\n",
    "    embedding = reducer.fit_transform(latents)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, s=3)\n",
    "    plt.title(\"UMAP\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space_umap(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_latent_space_pca(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_data = batch[0]\n",
    "            mu, _ = model.encoder(batch_data.view(batch_data.size(0), -1))\n",
    "            latents.append(mu)\n",
    "    latents = torch.cat(latents).numpy()\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(latents)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(principalComponents[:, 0], principalComponents[:, 1], alpha=0.5, s=3)\n",
    "    plt.title(\"PCA\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space_pca(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-sne\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_latent_space_tsne(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_data = batch[0]\n",
    "            mu, _ = model.encoder(batch_data.view(batch_data.size(0), -1))\n",
    "            latents.append(mu)\n",
    "    latents = torch.cat(latents).numpy()\n",
    "    tsne = TSNE(n_components=2)\n",
    "    tsne_results = tsne.fit_transform(latents)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(tsne_results[:, 0], tsne_results[:, 1], alpha=0.5, s=3)\n",
    "    plt.title(\"t-SNE\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space_tsne(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_latent_space_heatmap(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_data = batch[0]\n",
    "            mu, _ = model.encoder(batch_data.view(batch_data.size(0), -1))\n",
    "            latents.append(mu)\n",
    "    latents = torch.cat(latents).numpy()\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.heatmap(latents)\n",
    "    plt.title(\"Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space_heatmap(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plots  - right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "\n",
    "import umap\n",
    "\n",
    "def visualize_latent_space_umap(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_data = batch[0]\n",
    "            mu, _ = model.encoder(batch_data.view(batch_data.size(0), -1))\n",
    "            latents.append(mu)\n",
    "    latents = torch.cat(latents).numpy()\n",
    "    reducer = umap.UMAP()\n",
    "    embedding = reducer.fit_transform(latents)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, s=3)\n",
    "    plt.title(\"UMAP\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space_umap(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_latent_space_pca(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_data = batch[0]\n",
    "            mu, _ = model.encoder(batch_data.view(batch_data.size(0), -1))\n",
    "            latents.append(mu)\n",
    "    latents = torch.cat(latents).numpy()\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(latents)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(principalComponents[:, 0], principalComponents[:, 1], alpha=0.5, s=3)\n",
    "    plt.title(\"PCA\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space_pca(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-sne\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_latent_space_tsne(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_data = batch[0]\n",
    "            mu, _ = model.encoder(batch_data.view(batch_data.size(0), -1))\n",
    "            latents.append(mu)\n",
    "    latents = torch.cat(latents).numpy()\n",
    "    tsne = TSNE(n_components=2)\n",
    "    tsne_results = tsne.fit_transform(latents)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(tsne_results[:, 0], tsne_results[:, 1], alpha=0.5, s=3)\n",
    "    plt.title(\"t-SNE\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space_tsne(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_latent_space_heatmap(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_data = batch[0]\n",
    "            mu, _ = model.encoder(batch_data.view(batch_data.size(0), -1))\n",
    "            latents.append(mu)\n",
    "    latents = torch.cat(latents).numpy()\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.heatmap(latents)\n",
    "    plt.title(\"Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space_heatmap(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            batch_data = batch[0]  # get the data from the batch\n",
    "            batch_data = batch_data.view(batch_data.size(0), -1)  # flatten the data\n",
    "            reconstructed_batch, _, _ = model(batch_data)  # get reconstructed data from the model\n",
    "\n",
    "            # Convert tensors to numpy arrays for use in pandas\n",
    "            original_data = batch_data.detach().cpu().numpy()\n",
    "            reconstructed_data = reconstructed_batch.detach().cpu().numpy()\n",
    "\n",
    "            # Convert to dataframes\n",
    "            original_df = pd.DataFrame(original_data)\n",
    "            reconstructed_df = pd.DataFrame(reconstructed_data)\n",
    "\n",
    "            if i == 0:  # for the first iteration, create the dataframes\n",
    "                all_original_df = original_df\n",
    "                all_reconstructed_df = reconstructed_df\n",
    "            else:  # for subsequent iterations, append to the existing dataframes\n",
    "                all_original_df = pd.concat([all_original_df, original_df])\n",
    "                all_reconstructed_df = pd.concat([all_reconstructed_df, reconstructed_df])\n",
    "    \n",
    "    return all_original_df, all_reconstructed_df\n",
    "\n",
    "# Call the function after training\n",
    "original_df, reconstructed_df = test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df, reconstructed_df = test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first row from the dataframe\n",
    "first_row = reconstructed_df.iloc[0]\n",
    "\n",
    "# Reshape it to (1000, 10)\n",
    "reshaped_array = np.reshape(first_row.values, (1000, 10))\n",
    "\n",
    "# Convert it back to a dataframe\n",
    "reshaped_df = pd.DataFrame(reshaped_array)\n",
    "recon_combined_tensor = torch.tensor(reshaped_df.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay(reshaped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first row from the dataframe\n",
    "first_row = original_df.iloc[0]\n",
    "\n",
    "# Reshape it to (1000, 10)\n",
    "reshaped_array = np.reshape(first_row.values, (1000, 10))\n",
    "\n",
    "# Convert it back to a dataframe\n",
    "reshaped_df = pd.DataFrame(reshaped_array)\n",
    "ori_combined_tensor = torch.tensor(reshaped_df.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay(reshaped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_combined_tensor-ori_combined_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the tensors\n",
    "flattened_ori = ori_combined_tensor.flatten()\n",
    "flattened_recon = recon_combined_tensor.flatten()\n",
    "\n",
    "# Calculate the Euclidean distance\n",
    "euclidean_distance = torch.norm(flattened_ori - flattened_recon)\n",
    "\n",
    "\n",
    "print(euclidean_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class VRNN(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, z_dim, n_layers, dropout=0.):\n",
    "        super(VRNN, self).__init__()\n",
    "\n",
    "        self.x_dim = x_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.encoder = nn.LSTM(x_dim + h_dim, z_dim, n_layers, dropout=dropout)\n",
    "        self.decoder = nn.LSTM(z_dim + h_dim, x_dim, n_layers, dropout=dropout)\n",
    "        self.hidden_to_hidden = nn.Linear(z_dim, h_dim)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.zeros(self.n_layers, x.size(1), self.h_dim).to(x.device)\n",
    "\n",
    "        for t in range(x.size(0)):\n",
    "            _, h = self.encoder(torch.cat([x[t], h[-1]], -1).unsqueeze(0), h)\n",
    "            mu, log_var = h.chunk(2, -1)\n",
    "            z = self.reparameterize(mu, log_var)\n",
    "            _, h = self.decoder(torch.cat([z, h[-1]], -1).unsqueeze(0), h)\n",
    "            h = self.hidden_to_hidden(h)\n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data loader\n",
    "batch_size = 512  # adjust as necessary\n",
    "seq_length = 1000  # sequence length\n",
    "\n",
    "# Split data into train, validation, and test\n",
    "train_size = int(0.7 * len(dataset))  # 70% for training\n",
    "valid_size = int(0.15 * len(dataset))  # 15% for validation\n",
    "test_size = len(dataset) - train_size - valid_size  # 15% for testing\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "model = VRNN(x_dim=10, h_dim=128, z_dim=20, n_layers=2, dropout=0.2)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)  # Make sure you're using the correct optimizer\n",
    "loss_fn = nn.MSELoss()  # And the correct loss function\n",
    "\n",
    "# Training\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    for i, batch in enumerate(train_loader):  # using train_loader instead of dataloader\n",
    "        batch_data = batch  # get the data from the batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        h = model(batch_data)\n",
    "\n",
    "        # Loss: reconstruction loss + KL divergence\n",
    "        recon_loss = loss_fn(h, batch_data)\n",
    "        loss = recon_loss  # modify this line to include KL divergence\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'====> Epoch: {epoch} Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    batch_data = batch\n",
    "    print(batch_data.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# render with action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Convert combined_arr to PyTorch Tensor\n",
    "# combined_tensor = torch.from_numpy(combined_arr)\n",
    "\n",
    "# Print the shape of combined_tensor\n",
    "print(combined_tensor.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import gym\n",
    "\n",
    "\n",
    "def replay(combined_data):\n",
    "\n",
    "    action_sp = combined_data.iloc[:, :2]\n",
    "    obs_sp = combined_data.iloc[:, 2:]\n",
    "\n",
    "    env = gym.make('Swimmer-v3', render_mode = 'human')\n",
    "\n",
    "    # Iterate through the rows\n",
    "    for i in range(len(action_sp)):\n",
    "        # Get the i-th row\n",
    "        action = action_sp.iloc[i]\n",
    "        observation = obs_sp.iloc[i]\n",
    "        print(action)\n",
    "\n",
    "        # If this is the first iteration, set the environment state to the given observation\n",
    "        # Note: This assumes that the observation you've stored is the entire state that can be set with `env.reset()`\n",
    "        # If this is not the case, you cannot simply set the environment state to the observation\n",
    "        if i == 0:\n",
    "            env.reset()  # We ignore the initial observation returned by `reset`\n",
    "\n",
    "        # Apply the action\n",
    "        next_observation, reward, done, trunc, info = env.step(action)\n",
    "        # Render the environment\n",
    "        env.render()\n",
    "        # If you want to slow down each step for viewing, you can use time.sleep\n",
    "        # time.sleep(0.01)\n",
    "\n",
    "    # Close the environment\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
