{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import shape,math\n",
    "from tensorflow.keras import Input,layers,Model\n",
    "from tensorflow.keras.losses import mse,binary_crossentropy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import torch; torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 999, 10)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "output_dir = os.path.join(\".\", \"output\")  # Path to the output directory\n",
    "subdirs = [f.path for f in os.scandir(output_dir) if f.is_dir()]\n",
    "subdirs.sort()\n",
    "\n",
    "# Create an empty 3D array to store the combined data\n",
    "combined_arr = np.empty((len(subdirs), 999, 10))\n",
    "\n",
    "# Loop through each subdirectory and load the CSV files\n",
    "for i, subdir in enumerate(subdirs):\n",
    "    action_filename = os.path.join(subdir, \"action.csv\")\n",
    "    obs_filename = os.path.join(subdir, \"obs.csv\")\n",
    "\n",
    "    # Load the action and obs CSV files\n",
    "    action_df = pd.read_csv(action_filename)\n",
    "    obs_df = pd.read_csv(obs_filename)\n",
    "\n",
    "    # Concatenate the DataFrames horizontally\n",
    "    combined_data = pd.concat([action_df, obs_df], axis=1)\n",
    "\n",
    "    # Convert combined_data to a 3D array and assign it to combined_arr\n",
    "    combined_arr[i] = np.reshape(combined_data.values, (999, 10))\n",
    "\n",
    "# Print the shape of combined_arr\n",
    "print(combined_arr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_arr = combined_arr.reshape(10000, 9990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "combined_df = np.array(combined_arr)\n",
    "combined_tensor = torch.from_numpy(combined_df)\n",
    "flattened_tensor = combined_tensor.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 9990])\n",
      "tensor([[ 1.0000, -0.4887, -0.1252,  ...,  0.0083, -0.0401, -0.0155],\n",
      "        [ 1.0000,  0.2914, -0.1100,  ..., -0.0194, -0.0593, -0.0475],\n",
      "        [-0.4827, -0.0642,  0.0884,  ...,  0.0491, -0.0721,  0.0813],\n",
      "        ...,\n",
      "        [ 0.2111,  0.1188,  0.0484,  ...,  0.0955, -0.0528,  0.0854],\n",
      "        [ 0.9419,  0.2745, -0.0977,  ...,  0.0782, -0.0866, -0.0358],\n",
      "        [-0.7274, -0.2809, -0.0156,  ..., -0.0464, -0.0701,  0.0366]],\n",
      "       dtype=torch.float64, names=('Seed', 'Data'))\n"
     ]
    }
   ],
   "source": [
    "# Rename dimensions to 'Seed' and 'Data'\n",
    "flattened_tensor.names = (\"Seed\", \"Data\")\n",
    "\n",
    "# Now flattened_tensor will have dimensions named 'Seed' and 'Data'\n",
    "print(flattened_tensor.shape)  # Output: torch.Size([10000, 9990])\n",
    "print(flattened_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flattened_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(flattened_tensor)):\n\u001b[1;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(flattened_tensor[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m'\u001b[39m], flattened_tensor[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i)], c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot([\u001b[38;5;241m0\u001b[39m, flattened_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]], [np\u001b[38;5;241m.\u001b[39mpercentile(flattened_tensor[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i)], \u001b[38;5;241m25\u001b[39m), np\u001b[38;5;241m.\u001b[39mpercentile(flattened_tensor[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i)], \u001b[38;5;241m25\u001b[39m)],c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot([\u001b[38;5;241m0\u001b[39m, flattened_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]], [np\u001b[38;5;241m.\u001b[39mpercentile(flattened_tensor[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i)], \u001b[38;5;241m75\u001b[39m), np\u001b[38;5;241m.\u001b[39mpercentile(flattened_tensor[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i)], \u001b[38;5;241m75\u001b[39m)],c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Y Plot\n",
    "for i in range(1,len(flattened_tensor)):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(flattened_tensor['Data'], flattened_tensor['Y{}'.format(i)], c='black')\n",
    "    plt.plot([0, flattened_tensor.shape[0]], [np.percentile(flattened_tensor['Y{}'.format(i)], 25), np.percentile(flattened_tensor['Y{}'.format(i)], 25)],c='red')\n",
    "    plt.plot([0, flattened_tensor.shape[0]], [np.percentile(flattened_tensor['Y{}'.format(i)], 75), np.percentile(flattened_tensor['Y{}'.format(i)], 75)],c='blue')\n",
    "    plt.title('Y{}'.format(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000, 10)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "output_dir = os.path.join(\".\", \"output\")  # Path to the output directory\n",
    "subdirs = [f.path for f in os.scandir(output_dir) if f.is_dir()]\n",
    "subdirs.sort()\n",
    "\n",
    "# Create an empty 3D array to store the combined data\n",
    "combined_arr = np.empty((len(subdirs), 1000, 10))\n",
    "\n",
    "# Loop through each subdirectory and load the CSV files\n",
    "for i, subdir in enumerate(subdirs):\n",
    "    action_filename = os.path.join(subdir, \"action.csv\")\n",
    "    obs_filename = os.path.join(subdir, \"obs.csv\")\n",
    "\n",
    "    # Load the action and obs CSV files\n",
    "    action_df = pd.read_csv(action_filename,  header=None)\n",
    "    obs_df = pd.read_csv(obs_filename,  header=None)\n",
    "\n",
    "    # Concatenate the DataFrames horizontally\n",
    "    combined_data = pd.concat([action_df, obs_df], axis=1)\n",
    "\n",
    "    # Convert combined_data to a 3D array and assign it to combined_arr\n",
    "    combined_arr[i-1] = np.reshape(combined_data.values, (1000, 10))\n",
    "\n",
    "# Print the shape of combined_arr\n",
    "print(combined_arr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = np.array(combined_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.01561330e-01,  3.12052670e-04, -7.71057430e-02,  9.93219768e-02,\n",
       "       -4.72605934e-02, -4.67853317e-03,  3.17784149e-01, -2.76567454e-01,\n",
       "        4.65930404e-01, -3.84762646e-01])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_arr[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "# output_dir = os.path.join(\".\", \"output\")  # Path to the output directory\n",
    "# subdirs = [f.path for f in os.scandir(output_dir) if f.is_dir()]\n",
    "# combined_df = pd.DataFrame()\n",
    "\n",
    "# # Create a tqdm progress bar\n",
    "# pbar = tqdm(subdirs, desc=\"Processing subdirectories\", unit=\"subdir\")\n",
    "\n",
    "# for subdir in pbar:\n",
    "#     action_filename = os.path.join(subdir, \"action.csv\")\n",
    "#     obs_filename = os.path.join(subdir, \"obs.csv\")\n",
    "#     action_df = pd.read_csv(action_filename)\n",
    "#     obs_df = pd.read_csv(obs_filename)\n",
    "#     combined_data = pd.concat([action_df, obs_df], axis=1)\n",
    "#     if combined_data.shape == (999,10):\n",
    "#         combined_df = pd.concat([combined_df, combined_data], ignore_index=True)\n",
    "#     else:\n",
    "#         print(\"fail : \", subdir)\n",
    "\n",
    "#     # Update the progress bar\n",
    "#     pbar.set_postfix({\"Processed subdirectories\": subdir})\n",
    "\n",
    "# # Close the progress bar\n",
    "# pbar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1000, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Convert combined_arr to PyTorch Tensor\n",
    "combined_tensor = torch.from_numpy(combined_arr)\n",
    "\n",
    "# Print the shape of combined_tensor\n",
    "print(combined_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# Assuming combined_tensor is your data\n",
    "# Convert the data to float32\n",
    "dataset = TensorDataset(combined_tensor.float())\n",
    "\n",
    "# Define the data loader\n",
    "batch_size = 256  # adjust as necessary\n",
    "\n",
    "# Split data into train, validation, and test\n",
    "train_size = int(0.7 * len(dataset))  # 70% for training\n",
    "valid_size = int(0.15 * len(dataset))  # 15% for validation\n",
    "test_size = len(dataset) - train_size - valid_size  # 15% for testing\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Hyperparameters\n",
    "input_shape = combined_tensor.shape[1] * combined_tensor.shape[2]  # modify this to match your data\n",
    "hidden_dim1 = 128  # modify as needed\n",
    "hidden_dim2 = 256  # modify as needed\n",
    "hidden_dim3 = 512  # modify as needed\n",
    "hidden_dim4 = 1024  # modify as needed\n",
    "hidden_dim5 = 2048  # modify as needed\n",
    "latent_dim = 10  # modify as needed\n",
    "lr = 3e-6  # learning rate\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_shape, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim3, hidden_dim4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim4, hidden_dim5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim5, latent_dim * 2)  # mean and variance\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim3, hidden_dim4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim4, hidden_dim5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim5, input_shape),\n",
    "            nn.Sigmoid()      ## 시그모이드를 뺴야됨. 왜냐면 애초데이터가 0과 1사이가 아니기때문\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = h.chunk(2, dim=1)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, mu, log_var\n",
    "    \n",
    "# Model, optimizer, and loss function\n",
    "model = VAE(input_shape, hidden_dim1, latent_dim)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.BCELoss(reduction='sum') #MSE로 바꿔보자.\n",
    "\n",
    "def train(epoch, model, optimizer, loss_fn, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kl_div = 0\n",
    "    for i, batch in enumerate(train_loader):  # using train_loader instead of dataloader\n",
    "        batch_data = batch[0]  # get the data from the batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Flatten the data\n",
    "        batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "\n",
    "        reconstructed_batch, mu, log_var = model(batch_data)\n",
    "\n",
    "        # Loss: reconstruction loss + KL divergence\n",
    "        recon_loss = loss_fn(reconstructed_batch, batch_data)\n",
    "        kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        loss = recon_loss + kl_divergence\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_kl_div += kl_divergence.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_div = total_kl_div / len(train_loader.dataset)\n",
    "    print(f'====> Epoch: {epoch} Average loss: {avg_loss}, Recon Loss: {avg_recon_loss}, KL Div: {avg_kl_div}')\n",
    "\n",
    "    return avg_loss, avg_recon_loss, avg_kl_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 6926.3250178571425, Recon Loss: 6926.322705357143, KL Div: 0.0022642139324120115\n",
      "====> Epoch: 2 Average loss: 6913.907705357143, Recon Loss: 6913.896571428571, KL Div: 0.011128255435398646\n",
      "====> Epoch: 3 Average loss: 6897.0861160714285, Recon Loss: 6897.001223214285, KL Div: 0.08491189473015921\n",
      "====> Epoch: 4 Average loss: 6867.868517857143, Recon Loss: 6867.328178571429, KL Div: 0.5403362857273647\n",
      "====> Epoch: 5 Average loss: 6799.024741071428, Recon Loss: 6796.193875, KL Div: 2.8308433968680244\n",
      "====> Epoch: 6 Average loss: 6580.820116071429, Recon Loss: 6568.127339285715, KL Div: 12.692796456473214\n",
      "====> Epoch: 7 Average loss: 5639.0133214285715, Recon Loss: 5562.045830357143, KL Div: 76.96753473772321\n",
      "====> Epoch: 8 Average loss: -833.3449196428571, Recon Loss: -4372.614466517857, KL Div: 3539.26953515625\n",
      "====> Epoch: 9 Average loss: -31755.60532142857, Recon Loss: -48316.5135, KL Div: 16560.908035714285\n",
      "====> Epoch: 10 Average loss: -87201.00285714286, Recon Loss: -107009.48042857143, KL Div: 19808.477089285716\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# modify as needed\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     avg_loss, avg_recon_loss, avg_kl_div \u001b[38;5;241m=\u001b[39m train(epoch, model, optimizer, loss_fn, train_loader)\n\u001b[1;32m     12\u001b[0m     avg_losses\u001b[38;5;241m.\u001b[39mappend(avg_loss)\n\u001b[1;32m     13\u001b[0m     avg_recon_losses\u001b[38;5;241m.\u001b[39mappend(avg_recon_loss)\n",
      "Cell \u001b[0;32mIn[133], line 107\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, model, optimizer, loss_fn, train_loader)\u001b[0m\n\u001b[1;32m    104\u001b[0m kl_divergence \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m log_var \u001b[38;5;241m-\u001b[39m mu\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m log_var\u001b[38;5;241m.\u001b[39mexp())\n\u001b[1;32m    105\u001b[0m loss \u001b[38;5;241m=\u001b[39m recon_loss \u001b[38;5;241m+\u001b[39m kl_divergence\n\u001b[0;32m--> 107\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    108\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    109\u001b[0m total_recon_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m recon_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/firstenv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/firstenv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lists to store losses for each epoch\n",
    "avg_losses = []\n",
    "avg_recon_losses = []\n",
    "avg_kl_divs = []\n",
    "\n",
    "# Training\n",
    "n_epochs = 100  # modify as needed\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    avg_loss, avg_recon_loss, avg_kl_div = train(epoch, model, optimizer, loss_fn, train_loader)\n",
    "    avg_losses.append(avg_loss)\n",
    "    avg_recon_losses.append(avg_recon_loss)\n",
    "    avg_kl_divs.append(avg_kl_div)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(avg_losses, label='Average Loss')\n",
    "plt.plot(avg_recon_losses, label='Reconstruction Loss')\n",
    "plt.plot(avg_kl_divs, label='KL Divergence')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kl_div = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            batch_data = batch[0]  # get the data from the batch\n",
    "\n",
    "            # Flatten the data\n",
    "            batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "\n",
    "            reconstructed_batch, mu, log_var = model(batch_data)\n",
    "\n",
    "            # Loss: reconstruction loss + KL divergence\n",
    "            recon_loss = loss_fn(reconstructed_batch, batch_data)\n",
    "            kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "            loss += recon_loss + kl_divergence\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_kl_div += kl_divergence.item()\n",
    "\n",
    "    avg_loss = loss / len(dataloader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(dataloader.dataset)\n",
    "    avg_kl_div = total_kl_div / len(dataloader.dataset)\n",
    "\n",
    "    return avg_loss, avg_recon_loss, avg_kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-134727.6719), -150761.06666666668, 16033.394833333334)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, loss_fn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
