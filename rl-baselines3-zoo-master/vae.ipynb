{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import shape,math\n",
    "from tensorflow.keras import Input,layers,Model\n",
    "from tensorflow.keras.losses import mse,binary_crossentropy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import torch; torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "latent_dims = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create two sample DataFrames\n",
    "df1 = pd.DataFrame([[1, 2, 3], [4, 5, 6]])\n",
    "df2 = pd.DataFrame([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Concatenate the DataFrames horizontally\n",
    "combined_df = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "# Reshape the combined DataFrame into (2, 2, 3)\n",
    "reshaped_df = np.reshape(combined_df.values, (2, 2, 3))\n",
    "\n",
    "# Print the reshaped DataFrame\n",
    "print(reshaped_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./output/20230714_115330', './output/20230714_115045', './output/20230714_115149']\n",
      "(3, 999, 10)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "output_dir = os.path.join(\".\", \"output\")  # Path to the output directory\n",
    "\n",
    "# Get a list of subdirectories (timestamped folders) in the output directory\n",
    "subdirs = [f.path for f in os.scandir(output_dir) if f.is_dir()]\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "combined_df = pd.DataFrame()\n",
    "print(subdirs)\n",
    "\n",
    "# Loop through each subdirectory and load the CSV files\n",
    "for subdir in subdirs:\n",
    "    action_filename = os.path.join(subdir, \"action.csv\")\n",
    "    obs_filename = os.path.join(subdir, \"obs.csv\")\n",
    "\n",
    "    # Load the action and obs CSV files\n",
    "    action_df = pd.read_csv(action_filename)\n",
    "    obs_df = pd.read_csv(obs_filename)\n",
    "\n",
    "    # Concatenate the DataFrames horizontally\n",
    "    combined_data = pd.concat([action_df, obs_df], axis=1)\n",
    "    # combined_data['FileName'] = os.path.basename(subdir)\n",
    "\n",
    "    # Append the combined data to the combined DataFrame\n",
    "    combined_df = pd.concat([combined_df, combined_data], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "combined_df = np.reshape(combined_df.values, (len(subdirs), 999, 10))\n",
    "\n",
    "\n",
    "# Print the combined DataFrame\n",
    "print(combined_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.        , -0.4887499 , -0.12518893, ..., -1.48101765,\n",
       "          3.16040391, -2.97962481],\n",
       "        [ 1.        ,  0.6673651 , -0.19520388, ..., -1.98274603,\n",
       "          3.5745104 , -2.21610183],\n",
       "        [ 1.        ,  1.        , -0.27878554, ..., -2.18284202,\n",
       "          3.36718039, -0.84459476],\n",
       "        ...,\n",
       "        [-1.        , -1.        , -0.26966133, ...,  2.38654608,\n",
       "         -3.43401684, -0.63860864],\n",
       "        [-1.        , -1.        , -0.17271106, ...,  2.45474947,\n",
       "         -3.33365148, -1.16270128],\n",
       "        [-1.        , -1.        ,  0.07148086, ...,  0.00829224,\n",
       "         -0.04005762, -0.01546256]],\n",
       "\n",
       "       [[ 1.        , -0.4887499 , -0.12518893, ..., -1.48101765,\n",
       "          3.16040391, -2.97962481],\n",
       "        [ 1.        ,  0.6673651 , -0.19520388, ..., -1.98274603,\n",
       "          3.5745104 , -2.21610183],\n",
       "        [ 1.        ,  1.        , -0.27878554, ..., -2.18284202,\n",
       "          3.36718039, -0.84459476],\n",
       "        ...,\n",
       "        [-1.        , -1.        , -0.26966133, ...,  2.38654608,\n",
       "         -3.43401684, -0.63860864],\n",
       "        [-1.        , -1.        , -0.17271106, ...,  2.45474947,\n",
       "         -3.33365148, -1.16270128],\n",
       "        [-1.        , -1.        ,  0.07148086, ...,  0.00829224,\n",
       "         -0.04005762, -0.01546256]],\n",
       "\n",
       "       [[ 1.        , -0.4887499 , -0.12518893, ..., -1.48101765,\n",
       "          3.16040391, -2.97962481],\n",
       "        [ 1.        ,  0.6673651 , -0.19520388, ..., -1.98274603,\n",
       "          3.5745104 , -2.21610183],\n",
       "        [ 1.        ,  1.        , -0.27878554, ..., -2.18284202,\n",
       "          3.36718039, -0.84459476],\n",
       "        ...,\n",
       "        [-1.        , -1.        , -0.26966133, ...,  2.38654608,\n",
       "         -3.43401684, -0.63860864],\n",
       "        [-1.        , -1.        , -0.17271106, ...,  2.45474947,\n",
       "         -3.33365148, -1.16270128],\n",
       "        [-1.        , -1.        ,  0.07148086, ...,  0.00829224,\n",
       "         -0.04005762, -0.01546256]]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "import numpy as np\n",
    "device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "latent_dims = 3\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(latent_dims, 512)\n",
    "        self.linear2 = nn.Linear(512, 2448)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        z = F.relu(self.linear1(z))\n",
    "        z = torch.sigmoid(self.linear2(z))\n",
    "        return z.reshape((3, 999, 10))\n",
    "\n",
    "\n",
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(9990,128)\n",
    "        self.linear2 = nn.Linear(128, latent_dims)\n",
    "        self.linear3 = nn.Linear(128, latent_dims)\n",
    "        \n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc = self.N.loc.to(device) # hack to get sampling on the GPU\n",
    "        self.N.scale = self.N.scale.to(device)\n",
    "        self.kl = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=2)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu =  self.linear2(x)\n",
    "        sigma = torch.exp(self.linear3(x))\n",
    "        \n",
    "        # Re-paramaterization trick !\n",
    "        z = mu + sigma*self.N.sample(mu.shape)\n",
    "\n",
    "        # Kullback-Leibler Divergence\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
    "        \n",
    "        return z\n",
    "    \n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = VariationalEncoder(latent_dims)\n",
    "        self.decoder = Decoder(latent_dims)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "def train(autoencoder, data, epochs=20):\n",
    "    opt = torch.optim.Adam(autoencoder.parameters())\n",
    "    for epoch in range(epochs):\n",
    "        for x in data:\n",
    "            x = x.to(device) # GPU\n",
    "            opt.zero_grad()\n",
    "            x_hat = autoencoder(x)\n",
    "            loss = ((x - x_hat)**2).sum() + autoencoder.encoder.kl\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        print (f'Train epoch:{epoch} with previous loss {loss}');\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 999, 10]' is invalid for input of size 14688",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchsummary\u001b[39;00m \u001b[39mimport\u001b[39;00m summary\n\u001b[1;32m      2\u001b[0m vae \u001b[39m=\u001b[39m VariationalAutoencoder(latent_dims)\u001b[39m.\u001b[39mto(device) \u001b[39m# GPU\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m summary(vae, input_size\u001b[39m=\u001b[39mcombined_df\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/firstenv/lib/python3.11/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[39m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m model(\u001b[39m*\u001b[39mx)\n\u001b[1;32m     74\u001b[0m \u001b[39m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/firstenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[131], line 47\u001b[0m, in \u001b[0;36mVariationalAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     46\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x)\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(z)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/firstenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "Cell \u001b[0;32mIn[131], line 10\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m      8\u001b[0m z \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear1(z))\n\u001b[1;32m      9\u001b[0m z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(z))\n\u001b[0;32m---> 10\u001b[0m \u001b[39mreturn\u001b[39;00m z\u001b[39m.\u001b[39mreshape((\u001b[39m3\u001b[39m, \u001b[39m999\u001b[39m, \u001b[39m10\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[3, 999, 10]' is invalid for input of size 14688"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "vae = VariationalAutoencoder(latent_dims).to(device) # GPU\n",
    "summary(vae, input_size=combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m vae \u001b[39m=\u001b[39m VariationalAutoencoder(latent_dims)\u001b[39m.\u001b[39mto(device) \u001b[39m# GPU\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vae \u001b[39m=\u001b[39m train(vae, combined_df)\n",
      "Cell \u001b[0;32mIn[126], line 53\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(autoencoder, data, epochs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     52\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data:\n\u001b[0;32m---> 53\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device) \u001b[39m# GPU\u001b[39;00m\n\u001b[1;32m     54\u001b[0m         opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     55\u001b[0m         x_hat \u001b[39m=\u001b[39m autoencoder(x)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "vae = VariationalAutoencoder(latent_dims).to(device) # GPU\n",
    "vae = train(vae, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1           [-1, 2997, 5994]          71,928\n",
      "            Linear-2              [-1, 2997, 5]          29,975\n",
      "            Linear-3              [-1, 2997, 5]          29,975\n",
      "VariationalEncoder-4                    [-1, 5]               0\n",
      "            Linear-5                  [-1, 512]           3,072\n",
      "            Linear-6                 [-1, 2448]       1,255,824\n",
      "           Decoder-7                 [-1, 2448]               0\n",
      "================================================================\n",
      "Total params: 1,390,774\n",
      "Trainable params: 1,390,774\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.13\n",
      "Forward/backward pass size (MB): 137.32\n",
      "Params size (MB): 5.31\n",
      "Estimated Total Size (MB): 142.76\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(latent_dims, 512)\n",
    "        self.linear2 = nn.Linear(512, 2448)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        z = F.relu(self.linear1(z))\n",
    "        z = torch.sigmoid(self.linear2(z))\n",
    "        return z\n",
    "\n",
    "\n",
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(11, 5994)\n",
    "        self.linear2 = nn.Linear(5994, latent_dims)\n",
    "        self.linear3 = nn.Linear(5994, latent_dims)\n",
    "        \n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc = self.N.loc.to(device)  # hack to get sampling on the GPU\n",
    "        self.N.scale = self.N.scale.to(device)\n",
    "        self.kl = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=2)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu = self.linear2(x)\n",
    "        sigma = torch.exp(self.linear3(x))\n",
    "        \n",
    "        # Re-parameterization trick!\n",
    "        z = mu + sigma * self.N.sample(mu.shape)\n",
    "\n",
    "        # Kullback-Leibler Divergence\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
    "        \n",
    "        return z.reshape((-1, latent_dims))\n",
    "    \n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = VariationalEncoder(latent_dims)\n",
    "        self.decoder = Decoder(latent_dims)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "from torchsummary import summary\n",
    "vae = VariationalAutoencoder(latent_dims).to(device) # GPU\n",
    "summary(vae, input_size=combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
