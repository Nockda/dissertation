{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch; torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000, 10)\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(\".\", \"output_front_ppo\")  # Path to the output directory\n",
    "subdirs = [f.path for f in os.scandir(output_dir) if f.is_dir()]\n",
    "subdirs.sort()\n",
    "\n",
    "# Create an empty 3D array to store the combined data\n",
    "combined_arr_f = np.empty((len(subdirs), 1000, 10))\n",
    "\n",
    "# Loop through each subdirectory and load the CSV files\n",
    "for i, subdir in enumerate(subdirs):\n",
    "    action_filename = os.path.join(subdir, \"action.csv\")\n",
    "    obs_filename = os.path.join(subdir, \"obs.csv\")\n",
    "\n",
    "    # Load the action and obs CSV files\n",
    "    action_df = pd.read_csv(action_filename,  header=None)\n",
    "    obs_df = pd.read_csv(obs_filename,  header=None)\n",
    "\n",
    "    # Concatenate the DataFrames horizontally\n",
    "    combined_data = pd.concat([action_df, obs_df], axis=1)\n",
    "\n",
    "    # Convert combined_data to a 3D array and assign it to combined_arr\n",
    "    combined_arr_f[i-1] = np.reshape(combined_data.values, (1000, 10))\n",
    "\n",
    "# Print the shape of combined_arr\n",
    "print(combined_arr_f.shape)\n",
    "combined_tensor_f = torch.from_numpy(combined_arr_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000, 10)\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(\".\", \"output_bw_sac\")  # Path to the output directory\n",
    "subdirs = [f.path for f in os.scandir(output_dir) if f.is_dir()]\n",
    "subdirs.sort()\n",
    "\n",
    "# Create an empty 3D array to store the combined data\n",
    "combined_arr_b = np.empty((len(subdirs), 1000, 10))\n",
    "\n",
    "# Loop through each subdirectory and load the CSV files\n",
    "for i, subdir in enumerate(subdirs):\n",
    "    action_filename = os.path.join(subdir, \"action.csv\")\n",
    "    obs_filename = os.path.join(subdir, \"obs.csv\")\n",
    "\n",
    "    # Load the action and obs CSV files\n",
    "    action_df = pd.read_csv(action_filename,  header=None)\n",
    "    obs_df = pd.read_csv(obs_filename,  header=None)\n",
    "\n",
    "    # Concatenate the DataFrames horizontally\n",
    "    combined_data = pd.concat([action_df, obs_df], axis=1)\n",
    "\n",
    "    # Convert combined_data to a 3D array and assign it to combined_arr\n",
    "    combined_arr_b[i-1] = np.reshape(combined_data.values, (1000, 10))\n",
    "\n",
    "# Print the shape of combined_arr\n",
    "print(combined_arr_b.shape)\n",
    "combined_tensor_b = torch.from_numpy(combined_arr_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000, 10)\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(\".\", \"output_right_ppo\")  # Path to the output directory\n",
    "subdirs = [f.path for f in os.scandir(output_dir) if f.is_dir()]\n",
    "subdirs.sort()\n",
    "\n",
    "# Create an empty 3D array to store the combined data\n",
    "combined_arr_r = np.empty((len(subdirs), 1000, 10))\n",
    "\n",
    "# Loop through each subdirectory and load the CSV files\n",
    "for i, subdir in enumerate(subdirs):\n",
    "    action_filename = os.path.join(subdir, \"action.csv\")\n",
    "    obs_filename = os.path.join(subdir, \"obs.csv\")\n",
    "\n",
    "    # Load the action and obs CSV files\n",
    "    action_df = pd.read_csv(action_filename,  header=None)\n",
    "    obs_df = pd.read_csv(obs_filename,  header=None)\n",
    "\n",
    "    # Concatenate the DataFrames horizontally\n",
    "    combined_data = pd.concat([action_df, obs_df], axis=1)\n",
    "\n",
    "    # Convert combined_data to a 3D array and assign it to combined_arr\n",
    "    combined_arr_r[i-1] = np.reshape(combined_data.values, (1000, 10))\n",
    "\n",
    "# Print the shape of combined_arr\n",
    "print(combined_arr_r.shape)\n",
    "combined_tensor_r = torch.from_numpy(combined_arr_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000, 10)\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(\".\", \"output_jelly\")  # Path to the output directory\n",
    "subdirs = [f.path for f in os.scandir(output_dir) if f.is_dir()]\n",
    "subdirs.sort()\n",
    "\n",
    "# Create an empty 3D array to store the combined data\n",
    "combined_arr_j = np.empty((len(subdirs), 1000, 10))\n",
    "\n",
    "# Loop through each subdirectory and load the CSV files\n",
    "for i, subdir in enumerate(subdirs):\n",
    "    action_filename = os.path.join(subdir, \"action.csv\")\n",
    "    obs_filename = os.path.join(subdir, \"obs.csv\")\n",
    "\n",
    "    # Load the action and obs CSV files\n",
    "    action_df = pd.read_csv(action_filename,  header=None)\n",
    "    obs_df = pd.read_csv(obs_filename,  header=None)\n",
    "\n",
    "    # Concatenate the DataFrames horizontally\n",
    "    combined_data = pd.concat([action_df, obs_df], axis=1)\n",
    "\n",
    "    # Convert combined_data to a 3D array and assign it to combined_arr\n",
    "    combined_arr_j[i-1] = np.reshape(combined_data.values, (1000, 10))\n",
    "\n",
    "# Print the shape of combined_arr\n",
    "print(combined_arr_j.shape)\n",
    "combined_tensor_j = torch.from_numpy(combined_arr_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE code -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_shape, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim3, latent_dim * 2)  # mean and variance\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = h.chunk(2, dim=1)\n",
    "        return mu, log_var, h\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_shape):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim3, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, output_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_shape, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim1, hidden_dim2, hidden_dim3, input_shape)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var, h = self.encoder(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming combined_tensor is your data\n",
    "# Convert the data to float32\n",
    "datasetf = TensorDataset(combined_tensor_f.float())\n",
    "datasetb = TensorDataset(combined_tensor_b.float())\n",
    "datasetr = TensorDataset(combined_tensor_r.float())\n",
    "datasetj = TensorDataset(combined_tensor_j.float())\n",
    "# Define the data loader\n",
    "batch_size = 512  # adjust as necessary\n",
    " \n",
    "del combined_tensor_f\n",
    "del combined_tensor_b\n",
    "del combined_tensor_r\n",
    "# del combined_tensor_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test\n",
    "ftrain_size = int(0.7 * len(datasetf))  # 70% for training\n",
    "fvalid_size = int(0.15 * len(datasetf))  # 15% for validation\n",
    "ftest_size = len(datasetf) - ftrain_size - fvalid_size  # 15% for testing\n",
    "\n",
    "ftrain_dataset, fvalid_dataset, ftest_dataset = random_split(datasetf, [ftrain_size, fvalid_size, ftest_size])\n",
    "\n",
    "ftrain_loader = DataLoader(ftrain_dataset, batch_size=batch_size, shuffle=True)\n",
    "fvalid_loader = DataLoader(fvalid_dataset, batch_size=batch_size, shuffle=True)\n",
    "ftest_loader = DataLoader(ftest_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test\n",
    "btrain_size = int(0.7 * len(datasetb))  # 70% for training\n",
    "bvalid_size = int(0.15 * len(datasetb))  # 15% for validation\n",
    "btest_size = len(datasetb) - btrain_size - bvalid_size  # 15% for testing\n",
    "\n",
    "btrain_dataset, bvalid_dataset, btest_dataset = random_split(datasetb, [btrain_size, bvalid_size, btest_size])\n",
    "\n",
    "btrain_loader = DataLoader(btrain_dataset, batch_size=batch_size, shuffle=True)\n",
    "bvalid_loader = DataLoader(bvalid_dataset, batch_size=batch_size, shuffle=True)\n",
    "btest_loader = DataLoader(btest_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test\n",
    "rtrain_size = int(0.7 * len(datasetr))  # 70% for training\n",
    "rvalid_size = int(0.15 * len(datasetr))  # 15% for validation\n",
    "rtest_size = len(datasetr) - rtrain_size - rvalid_size  # 15% for testing\n",
    "\n",
    "rtrain_dataset, rvalid_dataset, rtest_dataset = random_split(datasetr, [rtrain_size, rvalid_size, rtest_size])\n",
    "\n",
    "rtrain_loader = DataLoader(rtrain_dataset, batch_size=batch_size, shuffle=True)\n",
    "rvalid_loader = DataLoader(rvalid_dataset, batch_size=batch_size, shuffle=True)\n",
    "rtest_loader = DataLoader(rtest_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test\n",
    "jtrain_size = int(0.7 * len(datasetj))  # 70% for training\n",
    "jvalid_size = int(0.15 * len(datasetj))  # 15% for validation\n",
    "jtest_size = len(datasetj) - jtrain_size - jvalid_size  # 15% for testing\n",
    "\n",
    "jtrain_dataset, jvalid_dataset, jtest_dataset = random_split(datasetj, [jtrain_size, jvalid_size, jtest_size])\n",
    "\n",
    "jtrain_loader = DataLoader(jtrain_dataset, batch_size=batch_size, shuffle=True)\n",
    "jvalid_loader = DataLoader(jvalid_dataset, batch_size=batch_size, shuffle=True)\n",
    "jtest_loader = DataLoader(jtest_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# input_shape = combined_tensor.shape[1] * combined_tensor.shape[2]  # modify this to match your data\n",
    "# hidden_dim1 = 128  # modify as needed\n",
    "# hidden_dim2 = 64  # modify as needed\n",
    "# hidden_dim3 = 24  # modify as needed\n",
    "# latent_dim = 2  # modify as needed\n",
    "# lr = 5e-5  # learning rate\n",
    "# n_epochs = 200  # modify as needed\n",
    "# beta = 0.2\n",
    "input_shape = combined_tensor_j.shape[1] * combined_tensor_j.shape[2]  # modify this to match your data\n",
    "hidden_dim1 = 24  # modify as needed\n",
    "hidden_dim2 = 12  # modify as needed\n",
    "hidden_dim3 = 12  # modify as needed\n",
    "latent_dim = 2  # modify as needed\n",
    "lr = 5e-5  # learning rate\n",
    "n_epochs = 200  # modify as needed\n",
    "beta = 0.2\n",
    "    \n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "model = VAE(input_shape, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim)\n",
    "\n",
    "optimizer = optim.RAdam(model.parameters(), lr=lr)  # Make sure you're using the correct optimizer\n",
    "loss_fn = nn.MSELoss()  # And the correct loss function\n",
    "\n",
    "\n",
    "def train(epoch, model, optimizer, loss_fn, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kl_div = 0\n",
    "    for i, batch in enumerate(train_loader):  # using train_loader instead of dataloader\n",
    "        batch_data = batch[0]  # get the data from the batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Flatten the data\n",
    "        batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "\n",
    "        reconstructed_batch, mu, log_var = model(batch_data)\n",
    "\n",
    "        # Loss: reconstruction loss + KL divergence\n",
    "        recon_loss = loss_fn(reconstructed_batch, batch_data)\n",
    "        kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        loss = recon_loss + beta*kl_divergence\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_kl_div += kl_divergence.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_div = total_kl_div / len(train_loader.dataset)\n",
    "    print(f'====> Epoch: {epoch} Average loss: {avg_loss}, Recon Loss: {avg_recon_loss}, KL Div: {avg_kl_div}')\n",
    "\n",
    "    return avg_loss, avg_recon_loss, avg_kl_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 0.013681015763963971, Recon Loss: 0.0020025973149708338, KL Div: 0.05839209120614188\n",
      "====> Epoch: 2 Average loss: 0.012337133748190744, Recon Loss: 0.0020027458667755127, KL Div: 0.05167193767002651\n",
      "====> Epoch: 3 Average loss: 0.011050561155591693, Recon Loss: 0.0020016995327813286, KL Div: 0.04524430683680943\n",
      "====> Epoch: 4 Average loss: 0.009776033844266618, Recon Loss: 0.002000562310218811, KL Div: 0.03887735639299665\n",
      "====> Epoch: 5 Average loss: 0.009363633292061942, Recon Loss: 0.001999508466039385, KL Div: 0.03682062353406634\n",
      "====> Epoch: 6 Average loss: 0.008955981867653983, Recon Loss: 0.001997053827558245, KL Div: 0.03479463904244559\n",
      "====> Epoch: 7 Average loss: 0.00854538140978132, Recon Loss: 0.001995659947395325, KL Div: 0.03274860722678048\n",
      "====> Epoch: 8 Average loss: 0.008111039638519286, Recon Loss: 0.0019935484783990044, KL Div: 0.030587456021990096\n",
      "====> Epoch: 9 Average loss: 0.0077098545006343295, Recon Loss: 0.0019912323951721193, KL Div: 0.028593109948294504\n",
      "====> Epoch: 10 Average loss: 0.007499675376074655, Recon Loss: 0.0019889488645962305, KL Div: 0.027553631918770926\n",
      "====> Epoch: 11 Average loss: 0.007404998813356672, Recon Loss: 0.0019871234127453396, KL Div: 0.027089376722063338\n",
      "====> Epoch: 12 Average loss: 0.007331996543066842, Recon Loss: 0.0019845454607691085, KL Div: 0.026737254551478795\n",
      "====> Epoch: 13 Average loss: 0.0072595269680023194, Recon Loss: 0.0019819893070629666, KL Div: 0.02638768754686628\n",
      "====> Epoch: 14 Average loss: 0.007187712635312761, Recon Loss: 0.0019794229183878217, KL Div: 0.026041448048182897\n",
      "====> Epoch: 15 Average loss: 0.007115297658102853, Recon Loss: 0.001976567600454603, KL Div: 0.025693649155753\n",
      "====> Epoch: 16 Average loss: 0.007042565856661115, Recon Loss: 0.001973871614251818, KL Div: 0.02534347016470773\n",
      "====> Epoch: 17 Average loss: 0.006968947172164917, Recon Loss: 0.0019708981599126543, KL Div: 0.024990244456699916\n",
      "====> Epoch: 18 Average loss: 0.006894939013889858, Recon Loss: 0.0019681663513183593, KL Div: 0.024633862631661552\n",
      "====> Epoch: 19 Average loss: 0.006819912399564471, Recon Loss: 0.0019650289160864694, KL Div: 0.024274417059762136\n",
      "====> Epoch: 20 Average loss: 0.006744433266775949, Recon Loss: 0.0019620632869856696, KL Div: 0.023911849975585937\n",
      "====> Epoch: 21 Average loss: 0.006668101412909372, Recon Loss: 0.001958818452698844, KL Div: 0.023546414375305177\n",
      "====> Epoch: 22 Average loss: 0.006591223308018276, Recon Loss: 0.0019556495887892585, KL Div: 0.023177868434361048\n",
      "====> Epoch: 23 Average loss: 0.0065136845793042865, Recon Loss: 0.001952393352985382, KL Div: 0.02280645602090018\n",
      "====> Epoch: 24 Average loss: 0.006435511316571917, Recon Loss: 0.001949047727244241, KL Div: 0.022432317529405867\n",
      "====> Epoch: 25 Average loss: 0.006356664487293788, Recon Loss: 0.0019455351574080332, KL Div: 0.02205564662388393\n",
      "====> Epoch: 26 Average loss: 0.006277275289808001, Recon Loss: 0.001941975976739611, KL Div: 0.02167649575642177\n",
      "====> Epoch: 27 Average loss: 0.006197412150246756, Recon Loss: 0.0019384155869483948, KL Div: 0.02129498270579747\n",
      "====> Epoch: 28 Average loss: 0.006116840669087001, Recon Loss: 0.0019346005575997488, KL Div: 0.02091120038713728\n",
      "====> Epoch: 29 Average loss: 0.006035823379244123, Recon Loss: 0.001930754601955414, KL Div: 0.020525343350001744\n",
      "====> Epoch: 30 Average loss: 0.0059542696475982666, Recon Loss: 0.0019267773457935877, KL Div: 0.020137460844857354\n",
      "====> Epoch: 31 Average loss: 0.0058720965044839046, Recon Loss: 0.001922644257545471, KL Div: 0.019747260774884905\n",
      "====> Epoch: 32 Average loss: 0.005789429289954049, Recon Loss: 0.0019183973499706814, KL Div: 0.019355159623282298\n",
      "====> Epoch: 33 Average loss: 0.005706249645778111, Recon Loss: 0.0019139863167490277, KL Div: 0.018961316108703614\n",
      "====> Epoch: 34 Average loss: 0.005622472524642944, Recon Loss: 0.001909296248640333, KL Div: 0.018565881184169224\n",
      "====> Epoch: 35 Average loss: 0.005538253954478672, Recon Loss: 0.0019045450687408447, KL Div: 0.018168544019971575\n",
      "====> Epoch: 36 Average loss: 0.005453570468085153, Recon Loss: 0.001899619426046099, KL Div: 0.01776975475038801\n",
      "====> Epoch: 37 Average loss: 0.005368374620165144, Recon Loss: 0.00189447477885655, KL Div: 0.017369498252868654\n",
      "====> Epoch: 38 Average loss: 0.005282678161348616, Recon Loss: 0.0018891984650066921, KL Div: 0.01696739843913487\n",
      "====> Epoch: 39 Average loss: 0.005196304695946829, Recon Loss: 0.0018834809746061053, KL Div: 0.016564118249075752\n",
      "====> Epoch: 40 Average loss: 0.005109584910529001, Recon Loss: 0.0018777159793036324, KL Div: 0.016159344264439175\n",
      "====> Epoch: 41 Average loss: 0.005022459472928728, Recon Loss: 0.0018718334691865104, KL Div: 0.015753129959106445\n",
      "====> Epoch: 42 Average loss: 0.004934711422239031, Recon Loss: 0.0018655410579272679, KL Div: 0.015345851693834577\n",
      "====> Epoch: 43 Average loss: 0.004846555607659476, Recon Loss: 0.0018590557490076338, KL Div: 0.01493749931880406\n",
      "====> Epoch: 44 Average loss: 0.004757812738418579, Recon Loss: 0.0018521902135440282, KL Div: 0.014528112479618617\n",
      "====> Epoch: 45 Average loss: 0.004668742486408779, Recon Loss: 0.0018451636178152902, KL Div: 0.014117894240788052\n",
      "====> Epoch: 46 Average loss: 0.004578752602849688, Recon Loss: 0.001837408857686179, KL Div: 0.013706718376704626\n",
      "====> Epoch: 47 Average loss: 0.004488929169518607, Recon Loss: 0.0018299624238695418, KL Div: 0.01329483359200614\n",
      "====> Epoch: 48 Average loss: 0.004398492080824715, Recon Loss: 0.0018219708800315856, KL Div: 0.012882605825151716\n",
      "====> Epoch: 49 Average loss: 0.004307393687111991, Recon Loss: 0.0018133766566004073, KL Div: 0.012470085144042969\n",
      "====> Epoch: 50 Average loss: 0.004216177684920175, Recon Loss: 0.0018046768307685853, KL Div: 0.012057503972734723\n",
      "====> Epoch: 51 Average loss: 0.004124661070959909, Recon Loss: 0.0017956265807151795, KL Div: 0.011645172391619002\n",
      "====> Epoch: 52 Average loss: 0.004031969564301627, Recon Loss: 0.0017852998461042131, KL Div: 0.011233347994940622\n",
      "====> Epoch: 53 Average loss: 0.0039401945897511075, Recon Loss: 0.0017757248878479005, KL Div: 0.01082234811782837\n",
      "====> Epoch: 54 Average loss: 0.003846955350467137, Recon Loss: 0.0017644469312259128, KL Div: 0.01041254186630249\n",
      "====> Epoch: 55 Average loss: 0.00375409015587398, Recon Loss: 0.0017532290305410114, KL Div: 0.010004305226462227\n",
      "====> Epoch: 56 Average loss: 0.003661445566586086, Recon Loss: 0.0017418302978788104, KL Div: 0.009598076105117799\n",
      "====> Epoch: 57 Average loss: 0.0035680734600339616, Recon Loss: 0.001729215213230678, KL Div: 0.009194291285106114\n",
      "====> Epoch: 58 Average loss: 0.0034746880190713067, Recon Loss: 0.0017159908499036517, KL Div: 0.008793485505240305\n",
      "====> Epoch: 59 Average loss: 0.003381364345550537, Recon Loss: 0.001702125677040645, KL Div: 0.00839619323185512\n",
      "====> Epoch: 60 Average loss: 0.003288233075823103, Recon Loss: 0.0016876635721751621, KL Div: 0.008002847330910818\n",
      "====> Epoch: 61 Average loss: 0.0031957965578351703, Recon Loss: 0.0016729551127978733, KL Div: 0.007614207199641636\n",
      "====> Epoch: 62 Average loss: 0.003104300124304635, Recon Loss: 0.0016581563268389021, KL Div: 0.007230718817029681\n",
      "====> Epoch: 63 Average loss: 0.003012677652495248, Recon Loss: 0.0016420607396534512, KL Div: 0.006853084496089391\n",
      "====> Epoch: 64 Average loss: 0.0029226602826799663, Recon Loss: 0.001626288626875196, KL Div: 0.006481858321598598\n",
      "====> Epoch: 65 Average loss: 0.0028317432062966484, Recon Loss: 0.0016081990940230234, KL Div: 0.006117720569883074\n",
      "====> Epoch: 66 Average loss: 0.0027424957411629814, Recon Loss: 0.0015902408105986459, KL Div: 0.005761274405888149\n",
      "====> Epoch: 67 Average loss: 0.0026555997133255005, Recon Loss: 0.0015729693004063198, KL Div: 0.005413151758057731\n",
      "====> Epoch: 68 Average loss: 0.0025690017427716936, Recon Loss: 0.0015542136515889848, KL Div: 0.005073940174920218\n",
      "====> Epoch: 69 Average loss: 0.002484194312776838, Recon Loss: 0.001535353354045323, KL Div: 0.004744204759597778\n",
      "====> Epoch: 70 Average loss: 0.002400586724281311, Recon Loss: 0.0015156705805233546, KL Div: 0.004424580642155239\n",
      "====> Epoch: 71 Average loss: 0.0023184842041560584, Recon Loss: 0.0014953783750534058, KL Div: 0.004115529230662754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 72 Average loss: 0.0022378287826265606, Recon Loss: 0.0014743381312915258, KL Div: 0.003817453248160226\n",
      "====> Epoch: 73 Average loss: 0.002157372679029192, Recon Loss: 0.0014511854989188058, KL Div: 0.0035309359175818306\n",
      "====> Epoch: 74 Average loss: 0.0020839241828237262, Recon Loss: 0.0014327055897031512, KL Div: 0.003256093042237418\n",
      "====> Epoch: 75 Average loss: 0.002007886350154877, Recon Loss: 0.0014092018263680594, KL Div: 0.0029934225508144925\n",
      "====> Epoch: 76 Average loss: 0.0019342122673988342, Recon Loss: 0.0013855513589722769, KL Div: 0.0027433044484683447\n",
      "====> Epoch: 77 Average loss: 0.00186564815895898, Recon Loss: 0.0013644706777163915, KL Div: 0.002505887372153146\n",
      "====> Epoch: 78 Average loss: 0.0017961726784706115, Recon Loss: 0.0013398935198783875, KL Div: 0.0022813957418714253\n",
      "====> Epoch: 79 Average loss: 0.0017295708060264588, Recon Loss: 0.001315611515726362, KL Div: 0.0020697964089257375\n",
      "====> Epoch: 80 Average loss: 0.0016661268728119987, Recon Loss: 0.0012918665153639657, KL Div: 0.0018713017531803676\n",
      "====> Epoch: 81 Average loss: 0.0016041533095496041, Recon Loss: 0.0012670020035334996, KL Div: 0.0016857564789908273\n",
      "====> Epoch: 82 Average loss: 0.00154831611258643, Recon Loss: 0.0012457184706415449, KL Div: 0.0015129882054669517\n",
      "====> Epoch: 83 Average loss: 0.001489640176296234, Recon Loss: 0.0012194772703307015, KL Div: 0.0013508143893310003\n",
      "====> Epoch: 84 Average loss: 0.0014339662619999476, Recon Loss: 0.00119510532276971, KL Div: 0.0011943047089236123\n",
      "====> Epoch: 85 Average loss: 0.001377545884677342, Recon Loss: 0.0011687940614564078, KL Div: 0.0010437591373920441\n",
      "====> Epoch: 86 Average loss: 0.0013211127775056022, Recon Loss: 0.0011435190864971705, KL Div: 0.0008879684848444803\n",
      "====> Epoch: 87 Average loss: 0.0012684680308614458, Recon Loss: 0.0011191264646393912, KL Div: 0.0007467077544757298\n",
      "====> Epoch: 88 Average loss: 0.0012169456311634608, Recon Loss: 0.0010930602380207607, KL Div: 0.0006194269784859248\n",
      "====> Epoch: 89 Average loss: 0.0011705796037401472, Recon Loss: 0.0010695013914789472, KL Div: 0.0005053910953657967\n",
      "====> Epoch: 90 Average loss: 0.0011257992897714887, Recon Loss: 0.0010447490215301513, KL Div: 0.00040525129011699133\n",
      "====> Epoch: 91 Average loss: 0.0010835555791854857, Recon Loss: 0.0010197379589080811, KL Div: 0.0003190881099019732\n",
      "====> Epoch: 92 Average loss: 0.0010444666062082563, Recon Loss: 0.0009951206956590926, KL Div: 0.00024672954423086984\n",
      "====> Epoch: 93 Average loss: 0.001007932254246303, Recon Loss: 0.0009704256355762481, KL Div: 0.00018753308483532497\n",
      "====> Epoch: 94 Average loss: 0.0009745099416800908, Recon Loss: 0.0009464053085872106, KL Div: 0.0001405231441770281\n",
      "====> Epoch: 95 Average loss: 0.0009442073745386941, Recon Loss: 0.0009233581210885729, KL Div: 0.00010424629705292838\n",
      "====> Epoch: 96 Average loss: 0.0009157563788550241, Recon Loss: 0.0009002897824559893, KL Div: 7.733298199517387e-05\n",
      "====> Epoch: 97 Average loss: 0.0008908366944108691, Recon Loss: 0.0008793126770428248, KL Div: 5.762006981032235e-05\n",
      "====> Epoch: 98 Average loss: 0.0008621169243540083, Recon Loss: 0.0008533658002104078, KL Div: 4.375562071800232e-05\n",
      "====> Epoch: 99 Average loss: 0.0008385397962161473, Recon Loss: 0.0008316930702754429, KL Div: 3.4233595643724715e-05\n",
      "====> Epoch: 100 Average loss: 0.0008142288880688803, Recon Loss: 0.0008086793763296945, KL Div: 2.774755869592939e-05\n",
      "====> Epoch: 101 Average loss: 0.000793143127645765, Recon Loss: 0.0007884657638413565, KL Div: 2.3386742387499127e-05\n",
      "====> Epoch: 102 Average loss: 0.0007703377732208796, Recon Loss: 0.0007662343042237418, KL Div: 2.0517396075384957e-05\n",
      "====> Epoch: 103 Average loss: 0.0007507421714918954, Recon Loss: 0.0007470206277711051, KL Div: 1.860777395112174e-05\n",
      "====> Epoch: 104 Average loss: 0.000727150810616357, Recon Loss: 0.0007236877296652113, KL Div: 1.7315391983304705e-05\n",
      "====> Epoch: 105 Average loss: 0.0007086810256753649, Recon Loss: 0.0007054003987993513, KL Div: 1.6403134380068097e-05\n",
      "====> Epoch: 106 Average loss: 0.0006879715408597673, Recon Loss: 0.000684832853930337, KL Div: 1.5693443162100655e-05\n",
      "====> Epoch: 107 Average loss: 0.0006714755296707153, Recon Loss: 0.0006684547492436001, KL Div: 1.5103919165475028e-05\n",
      "====> Epoch: 108 Average loss: 0.0006501441853387015, Recon Loss: 0.0006472136165414537, KL Div: 1.4652856758662633e-05\n",
      "====> Epoch: 109 Average loss: 0.0006340846759932382, Recon Loss: 0.0006312307247093745, KL Div: 1.4269735131944928e-05\n",
      "====> Epoch: 110 Average loss: 0.0006159279133592334, Recon Loss: 0.0006131327109677451, KL Div: 1.3976020472390311e-05\n",
      "====> Epoch: 111 Average loss: 0.0005999175906181336, Recon Loss: 0.0005971797236374447, KL Div: 1.3689305101122174e-05\n",
      "====> Epoch: 112 Average loss: 0.000583281683070319, Recon Loss: 0.0005805815075125014, KL Div: 1.3500873531614032e-05\n",
      "====> Epoch: 113 Average loss: 0.0005684309814657484, Recon Loss: 0.0005657954684325627, KL Div: 1.3177586453301566e-05\n",
      "====> Epoch: 114 Average loss: 0.0005519213463578905, Recon Loss: 0.0005493411847523281, KL Div: 1.2900790997913906e-05\n",
      "====> Epoch: 115 Average loss: 0.0005370466709136962, Recon Loss: 0.0005345112936837332, KL Div: 1.2676911694662912e-05\n",
      "====> Epoch: 116 Average loss: 0.0005235360392502376, Recon Loss: 0.0005210403842585427, KL Div: 1.2478287730898176e-05\n",
      "====> Epoch: 117 Average loss: 0.0005104167567832129, Recon Loss: 0.0005079717657395772, KL Div: 1.2224950960704258e-05\n",
      "====> Epoch: 118 Average loss: 0.0004971208912985665, Recon Loss: 0.0004947180002927781, KL Div: 1.201445290020534e-05\n",
      "====> Epoch: 119 Average loss: 0.0004844784992081778, Recon Loss: 0.00048211382755211423, KL Div: 1.1823360409055437e-05\n",
      "====> Epoch: 120 Average loss: 0.0004733431807586125, Recon Loss: 0.00047102402363504684, KL Div: 1.1595768587929862e-05\n",
      "====> Epoch: 121 Average loss: 0.00046149948026452745, Recon Loss: 0.0004592259164367403, KL Div: 1.1367823396410262e-05\n",
      "====> Epoch: 122 Average loss: 0.0004506104567221233, Recon Loss: 0.0004483834952116013, KL Div: 1.113481606755938e-05\n",
      "====> Epoch: 123 Average loss: 0.00043989564265523636, Recon Loss: 0.00043771899810859135, KL Div: 1.0883212089538574e-05\n",
      "====> Epoch: 124 Average loss: 0.0004291849093777793, Recon Loss: 0.00042705428174563817, KL Div: 1.0653125388281687e-05\n",
      "====> Epoch: 125 Average loss: 0.00041902139144284385, Recon Loss: 0.0004169333577156067, KL Div: 1.0440170764923096e-05\n",
      "====> Epoch: 126 Average loss: 0.000410518935748509, Recon Loss: 0.0004084738812276295, KL Div: 1.0225287505558558e-05\n",
      "====> Epoch: 127 Average loss: 0.00040036219571317947, Recon Loss: 0.0003983584748847144, KL Div: 1.0018625429698399e-05\n",
      "====> Epoch: 128 Average loss: 0.0003927419462374278, Recon Loss: 0.0003907797655888966, KL Div: 9.81090750013079e-06\n",
      "====> Epoch: 129 Average loss: 0.0003847783846514566, Recon Loss: 0.00038285586663654874, KL Div: 9.612594332013812e-06\n",
      "====> Epoch: 130 Average loss: 0.0003765424660273961, Recon Loss: 0.00037465860801083703, KL Div: 9.41928369658334e-06\n",
      "====> Epoch: 131 Average loss: 0.00036824181463037217, Recon Loss: 0.0003663971424102783, KL Div: 9.223358971732003e-06\n",
      "====> Epoch: 132 Average loss: 0.00036092819699219295, Recon Loss: 0.0003591188043355942, KL Div: 9.046963282993862e-06\n",
      "====> Epoch: 133 Average loss: 0.0003547488898038864, Recon Loss: 0.00035297487676143644, KL Div: 8.870069469724383e-06\n",
      "====> Epoch: 134 Average loss: 0.0003474162348679134, Recon Loss: 0.0003456794960158212, KL Div: 8.683690002986363e-06\n",
      "====> Epoch: 135 Average loss: 0.00034075733593532016, Recon Loss: 0.00033905418004308427, KL Div: 8.5157539163317e-06\n",
      "====> Epoch: 136 Average loss: 0.0003341442133699145, Recon Loss: 0.00033245140526975904, KL Div: 8.464029857090542e-06\n",
      "====> Epoch: 137 Average loss: 0.0003290894755295345, Recon Loss: 0.00032745318114757536, KL Div: 8.181491068431309e-06\n",
      "====> Epoch: 138 Average loss: 0.0003235343417951039, Recon Loss: 0.00032193295444761, KL Div: 8.00692183630807e-06\n",
      "====> Epoch: 139 Average loss: 0.0003179592681782586, Recon Loss: 0.0003163892818348748, KL Div: 7.849931716918945e-06\n",
      "====> Epoch: 140 Average loss: 0.0003127251757042749, Recon Loss: 0.00031119499461991447, KL Div: 7.650899035590036e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 141 Average loss: 0.000308178329042026, Recon Loss: 0.00030667835686888016, KL Div: 7.499835320881435e-06\n",
      "====> Epoch: 142 Average loss: 0.00030218074790069036, Recon Loss: 0.00030071864809308734, KL Div: 7.310488394328526e-06\n",
      "====> Epoch: 143 Average loss: 0.0002980756078447614, Recon Loss: 0.00029664246737957, KL Div: 7.165687424795968e-06\n",
      "====> Epoch: 144 Average loss: 0.00029506439183439527, Recon Loss: 0.00029365806707314085, KL Div: 7.031632321221488e-06\n",
      "====> Epoch: 145 Average loss: 0.0002897283881902695, Recon Loss: 0.0002883536560194833, KL Div: 6.873658725193568e-06\n",
      "====> Epoch: 146 Average loss: 0.00028560943050043926, Recon Loss: 0.000284264617732593, KL Div: 6.724042551858084e-06\n",
      "====> Epoch: 147 Average loss: 0.00028251022739069804, Recon Loss: 0.0002811918726989201, KL Div: 6.591750042779105e-06\n",
      "====> Epoch: 148 Average loss: 0.00027843850425311495, Recon Loss: 0.00027714619891984124, KL Div: 6.461543696267264e-06\n",
      "====> Epoch: 149 Average loss: 0.0002745961930070605, Recon Loss: 0.0002733381177697863, KL Div: 6.290376186370849e-06\n",
      "====> Epoch: 150 Average loss: 0.0002701687514781952, Recon Loss: 0.00026893083325454166, KL Div: 6.189576217106411e-06\n",
      "====> Epoch: 151 Average loss: 0.000267867277775492, Recon Loss: 0.00026664399462086814, KL Div: 6.116424288068499e-06\n",
      "====> Epoch: 152 Average loss: 0.0002648049614259175, Recon Loss: 0.000263604998588562, KL Div: 5.999812058040074e-06\n",
      "====> Epoch: 153 Average loss: 0.0002622942083648273, Recon Loss: 0.0002611283628003938, KL Div: 5.829240594591413e-06\n",
      "====> Epoch: 154 Average loss: 0.00025903438138110297, Recon Loss: 0.00025789766545806613, KL Div: 5.683588130133492e-06\n",
      "====> Epoch: 155 Average loss: 0.0002556067598717553, Recon Loss: 0.0002544926404953003, KL Div: 5.570603268487113e-06\n",
      "====> Epoch: 156 Average loss: 0.00025382026178496223, Recon Loss: 0.00025273339343922477, KL Div: 5.434325763157436e-06\n",
      "====> Epoch: 157 Average loss: 0.00025102691671677997, Recon Loss: 0.00024996462251458847, KL Div: 5.311455045427595e-06\n",
      "====> Epoch: 158 Average loss: 0.0002483061913933073, Recon Loss: 0.0002472603480730738, KL Div: 5.22922192301069e-06\n",
      "====> Epoch: 159 Average loss: 0.0002452762903911727, Recon Loss: 0.0002442518600395748, KL Div: 5.122146436146327e-06\n",
      "====> Epoch: 160 Average loss: 0.0002429418595773833, Recon Loss: 0.00024194441842181343, KL Div: 4.987214292798723e-06\n",
      "====> Epoch: 161 Average loss: 0.00024109352912221637, Recon Loss: 0.0002401171912040029, KL Div: 4.881692784173148e-06\n",
      "====> Epoch: 162 Average loss: 0.0002390357947775296, Recon Loss: 0.00023807943718773977, KL Div: 4.7817826271057126e-06\n",
      "====> Epoch: 163 Average loss: 0.00023728505947760172, Recon Loss: 0.0002363493697983878, KL Div: 4.6784494604383195e-06\n",
      "====> Epoch: 164 Average loss: 0.00023583573422261646, Recon Loss: 0.00023491287337882178, KL Div: 4.6143148626599995e-06\n",
      "====> Epoch: 165 Average loss: 0.00023323161154985427, Recon Loss: 0.00023232836702040264, KL Div: 4.5162226472582135e-06\n",
      "====> Epoch: 166 Average loss: 0.0002307685666850635, Recon Loss: 0.00022988729604652948, KL Div: 4.4063499995640344e-06\n",
      "====> Epoch: 167 Average loss: 0.00022954160826546804, Recon Loss: 0.00022867429043565477, KL Div: 4.336591277803693e-06\n",
      "====> Epoch: 168 Average loss: 0.00022888433826821192, Recon Loss: 0.00022803113077368055, KL Div: 4.266027893338885e-06\n",
      "====> Epoch: 169 Average loss: 0.0002274291877235685, Recon Loss: 0.00022659070789813997, KL Div: 4.192394869668143e-06\n",
      "====> Epoch: 170 Average loss: 0.00022485991567373277, Recon Loss: 0.0002240310055868966, KL Div: 4.144553627286638e-06\n",
      "====> Epoch: 171 Average loss: 0.00022377270247255052, Recon Loss: 0.0002229610841189112, KL Div: 4.058092832565308e-06\n",
      "====> Epoch: 172 Average loss: 0.00022274066082068853, Recon Loss: 0.00022194482386112213, KL Div: 3.979184797831944e-06\n",
      "====> Epoch: 173 Average loss: 0.00022033201477357318, Recon Loss: 0.00021954819134303502, KL Div: 3.919120345796857e-06\n",
      "====> Epoch: 174 Average loss: 0.00021923407380070004, Recon Loss: 0.0002184637880751065, KL Div: 3.8514264992305215e-06\n",
      "====> Epoch: 175 Average loss: 0.00021807095727750233, Recon Loss: 0.00021731380905423846, KL Div: 3.7857464381626673e-06\n",
      "====> Epoch: 176 Average loss: 0.00021732583961316516, Recon Loss: 0.0002165750190615654, KL Div: 3.754092114312308e-06\n",
      "====> Epoch: 177 Average loss: 0.00021621862586055482, Recon Loss: 0.00021547374022858482, KL Div: 3.7244345460619245e-06\n",
      "====> Epoch: 178 Average loss: 0.0002148518626179014, Recon Loss: 0.00021412222726004465, KL Div: 3.6481746605464392e-06\n",
      "====> Epoch: 179 Average loss: 0.0002136279536145074, Recon Loss: 0.00021291304592575345, KL Div: 3.5745373794010705e-06\n",
      "====> Epoch: 180 Average loss: 0.00021250909034694945, Recon Loss: 0.00021180699339934758, KL Div: 3.5104794161660333e-06\n",
      "====> Epoch: 181 Average loss: 0.00021154751096452986, Recon Loss: 0.00021085837589842932, KL Div: 3.4456593649727956e-06\n",
      "====> Epoch: 182 Average loss: 0.00020918824523687363, Recon Loss: 0.00020850920145000732, KL Div: 3.3952125481196814e-06\n",
      "====> Epoch: 183 Average loss: 0.00020917507048164095, Recon Loss: 0.00020849911655698505, KL Div: 3.379757915224348e-06\n",
      "====> Epoch: 184 Average loss: 0.00020918840382780346, Recon Loss: 0.00020850678852626255, KL Div: 3.4080871513911654e-06\n",
      "====> Epoch: 185 Average loss: 0.00020693259473357882, Recon Loss: 0.0002062743157148361, KL Div: 3.291398286819458e-06\n",
      "====> Epoch: 186 Average loss: 0.00020662278894867215, Recon Loss: 0.00020597526324646814, KL Div: 3.237626382282802e-06\n",
      "====> Epoch: 187 Average loss: 0.00020665706587689263, Recon Loss: 0.00020601891832692284, KL Div: 3.190734556743077e-06\n",
      "====> Epoch: 188 Average loss: 0.00020487889860357558, Recon Loss: 0.0002042513777102743, KL Div: 3.13760553087507e-06\n",
      "====> Epoch: 189 Average loss: 0.00020441920629569462, Recon Loss: 0.00020380265372140066, KL Div: 3.0827650002070835e-06\n",
      "====> Epoch: 190 Average loss: 0.00020359300928456442, Recon Loss: 0.0002029843617762838, KL Div: 3.0432343482971193e-06\n",
      "====> Epoch: 191 Average loss: 0.00020314951241016388, Recon Loss: 0.00020254759277616227, KL Div: 3.0096002987452916e-06\n",
      "====> Epoch: 192 Average loss: 0.00020282185609851565, Recon Loss: 0.00020222920605114528, KL Div: 2.963240657533918e-06\n",
      "====> Epoch: 193 Average loss: 0.00020105450600385667, Recon Loss: 0.00020046876264469965, KL Div: 2.9287125383104596e-06\n",
      "====> Epoch: 194 Average loss: 0.00020039488800934383, Recon Loss: 0.00019981687090226582, KL Div: 2.8900887284960067e-06\n",
      "====> Epoch: 195 Average loss: 0.00020098343917301722, Recon Loss: 0.00020041350913899285, KL Div: 2.8496342045920237e-06\n",
      "====> Epoch: 196 Average loss: 0.00019955841239009586, Recon Loss: 0.00019899274302380426, KL Div: 2.828342573983329e-06\n",
      "====> Epoch: 197 Average loss: 0.00019804948568344117, Recon Loss: 0.0001974909326859883, KL Div: 2.7927586010524206e-06\n",
      "====> Epoch: 198 Average loss: 0.000198778506900583, Recon Loss: 0.0001982264199427196, KL Div: 2.760431596211025e-06\n",
      "====> Epoch: 199 Average loss: 0.00019741391709872654, Recon Loss: 0.00019686707109212875, KL Div: 2.7342353548322405e-06\n",
      "====> Epoch: 200 Average loss: 0.00019789229652711324, Recon Loss: 0.00019735023166452136, KL Div: 2.7103126049041747e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lists to store losses for each epoch\n",
    "favg_losses = []\n",
    "favg_recon_losses = []\n",
    "favg_kl_divs = []\n",
    "\n",
    "# Training\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    favg_loss, favg_recon_loss, favg_kl_div = train(epoch, model, optimizer, loss_fn, ftrain_loader)\n",
    "    favg_losses.append(favg_loss)\n",
    "    favg_recon_losses.append(favg_recon_loss)\n",
    "    favg_kl_divs.append(favg_kl_div)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(favg_losses, label='Average Loss')\n",
    "plt.plot(favg_recon_losses, label='Reconstruction Loss')\n",
    "plt.plot(favg_kl_divs, label='KL Divergence')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 0.010777041401181903, Recon Loss: 0.005909012930733817, KL Div: 0.024340142045702253\n",
      "====> Epoch: 2 Average loss: 0.007205849375043596, Recon Loss: 0.0055852735383169995, KL Div: 0.00810287880897522\n",
      "====> Epoch: 3 Average loss: 0.00659471777507237, Recon Loss: 0.005324483326503209, KL Div: 0.006351171834128244\n",
      "====> Epoch: 4 Average loss: 0.006283089331218175, Recon Loss: 0.005133389098303659, KL Div: 0.005748501096452986\n",
      "====> Epoch: 5 Average loss: 0.006042861461639405, Recon Loss: 0.0049881223951067245, KL Div: 0.005273695128304618\n",
      "====> Epoch: 6 Average loss: 0.005798498153686524, Recon Loss: 0.004871499708720616, KL Div: 0.004634991935321263\n",
      "====> Epoch: 7 Average loss: 0.005480643340519497, Recon Loss: 0.0047789306640625, KL Div: 0.003508563484464373\n",
      "====> Epoch: 8 Average loss: 0.005103321177618844, Recon Loss: 0.0046963391985212054, KL Div: 0.0020349098443984987\n",
      "====> Epoch: 9 Average loss: 0.0048103428568158834, Recon Loss: 0.004624521153313773, KL Div: 0.0009291086707796369\n",
      "====> Epoch: 10 Average loss: 0.0046715165206364226, Recon Loss: 0.004562916619437082, KL Div: 0.0005429996081760952\n",
      "====> Epoch: 11 Average loss: 0.004587364435195923, Recon Loss: 0.004510593550545829, KL Div: 0.0003838546403816768\n",
      "====> Epoch: 12 Average loss: 0.004516177824565343, Recon Loss: 0.004458063364028931, KL Div: 0.00029057236654417854\n",
      "====> Epoch: 13 Average loss: 0.004465401956013271, Recon Loss: 0.0044183965751102994, KL Div: 0.00023502679807799203\n",
      "====> Epoch: 14 Average loss: 0.004418105363845825, Recon Loss: 0.004377512693405151, KL Div: 0.0002029633607183184\n",
      "====> Epoch: 15 Average loss: 0.0043771721635546, Recon Loss: 0.004341156993593488, KL Div: 0.00018007531336375647\n",
      "====> Epoch: 16 Average loss: 0.004338198627744402, Recon Loss: 0.004305980954851423, KL Div: 0.00016108863268579756\n",
      "====> Epoch: 17 Average loss: 0.004306394815444947, Recon Loss: 0.004277294363294329, KL Div: 0.00014550259709358214\n",
      "====> Epoch: 18 Average loss: 0.00427191093989781, Recon Loss: 0.004245628118515015, KL Div: 0.00013141384295054846\n",
      "====> Epoch: 19 Average loss: 0.004244543654578073, Recon Loss: 0.004220448425837926, KL Div: 0.00012047617776053293\n",
      "====> Epoch: 20 Average loss: 0.004216803857258388, Recon Loss: 0.004194616079330444, KL Div: 0.0001109391450881958\n",
      "====> Epoch: 21 Average loss: 0.004189905268805368, Recon Loss: 0.004169351884296962, KL Div: 0.00010276712264333452\n",
      "====> Epoch: 22 Average loss: 0.004164451735360282, Recon Loss: 0.004145119803292411, KL Div: 9.665953261511667e-05\n",
      "====> Epoch: 23 Average loss: 0.004141497271401542, Recon Loss: 0.004123422486441476, KL Div: 9.037385242325919e-05\n",
      "====> Epoch: 24 Average loss: 0.004119773081370763, Recon Loss: 0.00410267676625933, KL Div: 8.548183952059065e-05\n",
      "====> Epoch: 25 Average loss: 0.0040996793338230674, Recon Loss: 0.004083546331950597, KL Div: 8.066529035568237e-05\n",
      "====> Epoch: 26 Average loss: 0.004078712259020124, Recon Loss: 0.004063510588237218, KL Div: 7.600843906402588e-05\n",
      "====> Epoch: 27 Average loss: 0.004056971447808402, Recon Loss: 0.004042540328843253, KL Div: 7.215557353837149e-05\n",
      "====> Epoch: 28 Average loss: 0.004040181687899999, Recon Loss: 0.004026450548853193, KL Div: 6.86558655330113e-05\n",
      "====> Epoch: 29 Average loss: 0.004020135232380458, Recon Loss: 0.00400703045300075, KL Div: 6.552392244338989e-05\n",
      "====> Epoch: 30 Average loss: 0.004003658175468445, Recon Loss: 0.003991102780614581, KL Div: 6.277699555669512e-05\n",
      "====> Epoch: 31 Average loss: 0.003986910837037223, Recon Loss: 0.00397483241558075, KL Div: 6.039200936044965e-05\n",
      "====> Epoch: 32 Average loss: 0.003974223358290536, Recon Loss: 0.003962544100625174, KL Div: 5.83963862487248e-05\n",
      "====> Epoch: 33 Average loss: 0.003953391824449812, Recon Loss: 0.003942085640771049, KL Div: 5.6530943938664025e-05\n",
      "====> Epoch: 34 Average loss: 0.0039373650891440255, Recon Loss: 0.003926393611090524, KL Div: 5.485733492033822e-05\n",
      "====> Epoch: 35 Average loss: 0.0039248274224145075, Recon Loss: 0.003914146644728524, KL Div: 5.340406724384853e-05\n",
      "====> Epoch: 36 Average loss: 0.003910843917301723, Recon Loss: 0.003900471602167402, KL Div: 5.18614479473659e-05\n",
      "====> Epoch: 37 Average loss: 0.003896304096494402, Recon Loss: 0.003886254276548113, KL Div: 5.0248976264681134e-05\n",
      "====> Epoch: 38 Average loss: 0.0038784382343292237, Recon Loss: 0.0038686719111033848, KL Div: 4.883163315909249e-05\n",
      "====> Epoch: 39 Average loss: 0.003865505405834743, Recon Loss: 0.0038560203995023456, KL Div: 4.742499760219029e-05\n",
      "====> Epoch: 40 Average loss: 0.003854412385395595, Recon Loss: 0.0038452135835375105, KL Div: 4.599404335021973e-05\n",
      "====> Epoch: 41 Average loss: 0.003840100015912737, Recon Loss: 0.0038312244415283202, KL Div: 4.437795281410217e-05\n",
      "====> Epoch: 42 Average loss: 0.003823824167251587, Recon Loss: 0.003815233043261937, KL Div: 4.295547093663897e-05\n",
      "====> Epoch: 43 Average loss: 0.0038127366815294537, Recon Loss: 0.003804405450820923, KL Div: 4.1656110967908584e-05\n",
      "====> Epoch: 44 Average loss: 0.003797655667577471, Recon Loss: 0.0037896089383534022, KL Div: 4.0233463048934936e-05\n",
      "====> Epoch: 45 Average loss: 0.0037897622414997645, Recon Loss: 0.0037819928441728866, KL Div: 3.884706326893398e-05\n",
      "====> Epoch: 46 Average loss: 0.0037747281789779663, Recon Loss: 0.003767211982182094, KL Div: 3.758096269198826e-05\n",
      "====> Epoch: 47 Average loss: 0.00376219916343689, Recon Loss: 0.0037549639599663872, KL Div: 3.617607269968305e-05\n",
      "====> Epoch: 48 Average loss: 0.003746981978416443, Recon Loss: 0.003740068486758641, KL Div: 3.45674284866878e-05\n",
      "====> Epoch: 49 Average loss: 0.0037360038076128276, Recon Loss: 0.0037293767418180193, KL Div: 3.313536303383964e-05\n",
      "====> Epoch: 50 Average loss: 0.003723662291254316, Recon Loss: 0.0037173404353005547, KL Div: 3.160926699638367e-05\n",
      "====> Epoch: 51 Average loss: 0.0037121392488479615, Recon Loss: 0.003706162009920393, KL Div: 2.9886203152792794e-05\n",
      "====> Epoch: 52 Average loss: 0.0036980256182806833, Recon Loss: 0.003692336814744132, KL Div: 2.844403897012983e-05\n",
      "====> Epoch: 53 Average loss: 0.0036895255020686557, Recon Loss: 0.003684126888002668, KL Div: 2.699305330004011e-05\n",
      "====> Epoch: 54 Average loss: 0.003677212544849941, Recon Loss: 0.0036720427445002964, KL Div: 2.5848954916000366e-05\n",
      "====> Epoch: 55 Average loss: 0.003665424874850682, Recon Loss: 0.0036604631117412023, KL Div: 2.4808858122144425e-05\n",
      "====> Epoch: 56 Average loss: 0.0036513394798551287, Recon Loss: 0.0036465480838503155, KL Div: 2.395703537123544e-05\n",
      "====> Epoch: 57 Average loss: 0.003641989162990025, Recon Loss: 0.0036373583929879324, KL Div: 2.3153918130057197e-05\n",
      "====> Epoch: 58 Average loss: 0.0036303391456604, Recon Loss: 0.0036258460112980435, KL Div: 2.246568032673427e-05\n",
      "====> Epoch: 59 Average loss: 0.0036190213135310584, Recon Loss: 0.003614667194230216, KL Div: 2.177054967199053e-05\n",
      "====> Epoch: 60 Average loss: 0.0036043890033449445, Recon Loss: 0.003600150040217808, KL Div: 2.1194824150630405e-05\n",
      "====> Epoch: 61 Average loss: 0.003595804878643581, Recon Loss: 0.003591698867934091, KL Div: 2.053024939128331e-05\n",
      "====> Epoch: 62 Average loss: 0.003584944656917027, Recon Loss: 0.003580984626497541, KL Div: 1.9800343683787755e-05\n",
      "====> Epoch: 63 Average loss: 0.0035748000144958498, Recon Loss: 0.0035709543739046367, KL Div: 1.9228458404541017e-05\n",
      "====> Epoch: 64 Average loss: 0.003563650335584368, Recon Loss: 0.0035599155596324375, KL Div: 1.8673943621771676e-05\n",
      "====> Epoch: 65 Average loss: 0.0035511482613427296, Recon Loss: 0.0035475058896200997, KL Div: 1.821176494870867e-05\n",
      "====> Epoch: 66 Average loss: 0.0035461717162813457, Recon Loss: 0.0035426536287580216, KL Div: 1.7590433359146117e-05\n",
      "====> Epoch: 67 Average loss: 0.0035320554290499007, Recon Loss: 0.0035286463499069214, KL Div: 1.704549789428711e-05\n",
      "====> Epoch: 68 Average loss: 0.00352806670325143, Recon Loss: 0.003524763584136963, KL Div: 1.6515659434454782e-05\n",
      "====> Epoch: 69 Average loss: 0.0035091416324887956, Recon Loss: 0.003505923288209098, KL Div: 1.6091721398489816e-05\n",
      "====> Epoch: 70 Average loss: 0.003498107671737671, Recon Loss: 0.003494981220790318, KL Div: 1.563236543110439e-05\n",
      "====> Epoch: 71 Average loss: 0.00348787750516619, Recon Loss: 0.003484825611114502, KL Div: 1.525956392288208e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 72 Average loss: 0.0034830473150525773, Recon Loss: 0.003480088574545724, KL Div: 1.4793791941234044e-05\n",
      "====> Epoch: 73 Average loss: 0.003470152088574001, Recon Loss: 0.0034672552176884244, KL Div: 1.4484448092324393e-05\n",
      "====> Epoch: 74 Average loss: 0.0034609474624906268, Recon Loss: 0.0034581338337489535, KL Div: 1.4068250145230975e-05\n",
      "====> Epoch: 75 Average loss: 0.0034490971054349628, Recon Loss: 0.0034463316202163695, KL Div: 1.3827379260744368e-05\n",
      "====> Epoch: 76 Average loss: 0.003438378589493888, Recon Loss: 0.0034356830971581597, KL Div: 1.3477440391268049e-05\n",
      "====> Epoch: 77 Average loss: 0.003434782385826111, Recon Loss: 0.0034321532930646622, KL Div: 1.3145531926836286e-05\n",
      "====> Epoch: 78 Average loss: 0.0034190643514905657, Recon Loss: 0.0034164984907422745, KL Div: 1.2829282454081944e-05\n",
      "====> Epoch: 79 Average loss: 0.003414249539375305, Recon Loss: 0.0034117329461233956, KL Div: 1.2582949229649135e-05\n",
      "====> Epoch: 80 Average loss: 0.003408507755824498, Recon Loss: 0.0034060630798339844, KL Div: 1.2223303318023682e-05\n",
      "====> Epoch: 81 Average loss: 0.0033950185775756835, Recon Loss: 0.003392632246017456, KL Div: 1.1931615216391428e-05\n",
      "====> Epoch: 82 Average loss: 0.003381132585661752, Recon Loss: 0.0033788075617381505, KL Div: 1.1625034468514578e-05\n",
      "====> Epoch: 83 Average loss: 0.0033744294132505145, Recon Loss: 0.0033721535546439034, KL Div: 1.1379233428410121e-05\n",
      "====> Epoch: 84 Average loss: 0.003366082651274545, Recon Loss: 0.003363872766494751, KL Div: 1.1049283402306694e-05\n",
      "====> Epoch: 85 Average loss: 0.0033530263900756834, Recon Loss: 0.003350867050034659, KL Div: 1.0796751294817243e-05\n",
      "====> Epoch: 86 Average loss: 0.0033468547889164515, Recon Loss: 0.0033447391646248955, KL Div: 1.0578257696969169e-05\n",
      "====> Epoch: 87 Average loss: 0.0033417109080723355, Recon Loss: 0.0033396391357694353, KL Div: 1.035896795136588e-05\n",
      "====> Epoch: 88 Average loss: 0.0033308438403265818, Recon Loss: 0.0033288191897528512, KL Div: 1.0123380592891148e-05\n",
      "====> Epoch: 89 Average loss: 0.0033210356065205165, Recon Loss: 0.0033190391404288155, KL Div: 9.982270853860038e-06\n",
      "====> Epoch: 90 Average loss: 0.0033116208825792583, Recon Loss: 0.0033096844128199985, KL Div: 9.68237008367266e-06\n",
      "====> Epoch: 91 Average loss: 0.003304702111652919, Recon Loss: 0.0033028002977371217, KL Div: 9.50900571686881e-06\n",
      "====> Epoch: 92 Average loss: 0.0032917501074927196, Recon Loss: 0.0032899029425212316, KL Div: 9.235892977033342e-06\n",
      "====> Epoch: 93 Average loss: 0.0032864600590297153, Recon Loss: 0.003284644433430263, KL Div: 9.078251464026316e-06\n",
      "====> Epoch: 94 Average loss: 0.0032732342651912143, Recon Loss: 0.0032714642116001675, KL Div: 8.850306272506713e-06\n",
      "====> Epoch: 95 Average loss: 0.0032644494942256383, Recon Loss: 0.003262705632618495, KL Div: 8.719367640359061e-06\n",
      "====> Epoch: 96 Average loss: 0.0032643546376909527, Recon Loss: 0.0032626459087644302, KL Div: 8.543602057865687e-06\n",
      "====> Epoch: 97 Average loss: 0.0032559270347867694, Recon Loss: 0.0032542498792920795, KL Div: 8.38566677910941e-06\n",
      "====> Epoch: 98 Average loss: 0.003254556025777544, Recon Loss: 0.0032528993572507586, KL Div: 8.28327877180917e-06\n",
      "====> Epoch: 99 Average loss: 0.003245971373149327, Recon Loss: 0.003244356291634696, KL Div: 8.075475692749024e-06\n",
      "====> Epoch: 100 Average loss: 0.003232067584991455, Recon Loss: 0.0032304842472076416, KL Div: 7.916671889168876e-06\n",
      "====> Epoch: 101 Average loss: 0.003225014873913356, Recon Loss: 0.0032234615768705096, KL Div: 7.766463926860264e-06\n",
      "====> Epoch: 102 Average loss: 0.0032187532697405133, Recon Loss: 0.0032172158445630756, KL Div: 7.687181234359742e-06\n",
      "====> Epoch: 103 Average loss: 0.0032084028720855712, Recon Loss: 0.0032069200447627476, KL Div: 7.4141068117959155e-06\n",
      "====> Epoch: 104 Average loss: 0.0032102280173982893, Recon Loss: 0.003208763156618391, KL Div: 7.3242655822208945e-06\n",
      "====> Epoch: 105 Average loss: 0.0031958905458450318, Recon Loss: 0.003194454993520464, KL Div: 7.177795682634626e-06\n",
      "====> Epoch: 106 Average loss: 0.0031878152574811663, Recon Loss: 0.003186417170933315, KL Div: 6.990564720971244e-06\n",
      "====> Epoch: 107 Average loss: 0.0031789753266743253, Recon Loss: 0.003177589757101876, KL Div: 6.927843604769026e-06\n",
      "====> Epoch: 108 Average loss: 0.003180542196546282, Recon Loss: 0.003179186054638454, KL Div: 6.780726569039481e-06\n",
      "====> Epoch: 109 Average loss: 0.0031669011967522757, Recon Loss: 0.0031655734266553604, KL Div: 6.638893059321812e-06\n",
      "====> Epoch: 110 Average loss: 0.0031673483507973807, Recon Loss: 0.0031660379001072474, KL Div: 6.5522023609706335e-06\n",
      "====> Epoch: 111 Average loss: 0.0031601349455969673, Recon Loss: 0.0031588468040738785, KL Div: 6.440707615443639e-06\n",
      "====> Epoch: 112 Average loss: 0.003146236402647836, Recon Loss: 0.003144974742616926, KL Div: 6.308291639600481e-06\n",
      "====> Epoch: 113 Average loss: 0.003151510340826852, Recon Loss: 0.003150283132280622, KL Div: 6.136149168014527e-06\n",
      "====> Epoch: 114 Average loss: 0.003138689569064549, Recon Loss: 0.003137473685400827, KL Div: 6.079392773764474e-06\n",
      "====> Epoch: 115 Average loss: 0.0031366411447525024, Recon Loss: 0.0031354561022349766, KL Div: 5.925272192273821e-06\n",
      "====> Epoch: 116 Average loss: 0.00312839937210083, Recon Loss: 0.003127227851322719, KL Div: 5.8575570583343505e-06\n",
      "====> Epoch: 117 Average loss: 0.003118460518973214, Recon Loss: 0.0031173290184565955, KL Div: 5.657540900366647e-06\n",
      "====> Epoch: 118 Average loss: 0.0031115140574319022, Recon Loss: 0.0031103966747011457, KL Div: 5.586785929543631e-06\n",
      "====> Epoch: 119 Average loss: 0.003114388976778303, Recon Loss: 0.0031132991995130267, KL Div: 5.44884375163487e-06\n",
      "====> Epoch: 120 Average loss: 0.0031087972436632427, Recon Loss: 0.0031077161516462055, KL Div: 5.405404738017491e-06\n",
      "====> Epoch: 121 Average loss: 0.003103350809642247, Recon Loss: 0.003102299519947597, KL Div: 5.256486790520804e-06\n",
      "====> Epoch: 122 Average loss: 0.003094014814921788, Recon Loss: 0.0030929769277572633, KL Div: 5.189384732927595e-06\n",
      "====> Epoch: 123 Average loss: 0.0030985669238226756, Recon Loss: 0.0030975544793265205, KL Div: 5.062107528959002e-06\n",
      "====> Epoch: 124 Average loss: 0.0030834493637084963, Recon Loss: 0.0030824410915374755, KL Div: 5.041518381663731e-06\n",
      "====> Epoch: 125 Average loss: 0.0030848028319222587, Recon Loss: 0.003083812134606498, KL Div: 4.953529153551374e-06\n",
      "====> Epoch: 126 Average loss: 0.003080400177410671, Recon Loss: 0.0030794263226645332, KL Div: 4.869316305433001e-06\n",
      "====> Epoch: 127 Average loss: 0.0030809179544448854, Recon Loss: 0.003079955526760646, KL Div: 4.812019211905343e-06\n",
      "====> Epoch: 128 Average loss: 0.0030690886122839794, Recon Loss: 0.003068144083023071, KL Div: 4.7226973942347935e-06\n",
      "====> Epoch: 129 Average loss: 0.0030682110616139, Recon Loss: 0.0030672799519130163, KL Div: 4.655412265232631e-06\n",
      "====> Epoch: 130 Average loss: 0.003058589049748012, Recon Loss: 0.0030576729944774084, KL Div: 4.580267838069371e-06\n",
      "====> Epoch: 131 Average loss: 0.0030616122654506138, Recon Loss: 0.003060721584728786, KL Div: 4.4531822204589846e-06\n",
      "====> Epoch: 132 Average loss: 0.0030527903182165964, Recon Loss: 0.0030518984283719745, KL Div: 4.4593385287693564e-06\n",
      "====> Epoch: 133 Average loss: 0.0030551895243780955, Recon Loss: 0.0030543059621538434, KL Div: 4.417832408632551e-06\n",
      "====> Epoch: 134 Average loss: 0.003050986119679042, Recon Loss: 0.003050131712641035, KL Div: 4.271928753171648e-06\n",
      "====> Epoch: 135 Average loss: 0.003042421119553702, Recon Loss: 0.0030415678024291993, KL Div: 4.266415323529924e-06\n",
      "====> Epoch: 136 Average loss: 0.003035391960825239, Recon Loss: 0.0030345527274268013, KL Div: 4.1961499622889925e-06\n",
      "====> Epoch: 137 Average loss: 0.003033467973981585, Recon Loss: 0.0030326473031725204, KL Div: 4.103337015424456e-06\n",
      "====> Epoch: 138 Average loss: 0.0030293732370649065, Recon Loss: 0.003028556840760367, KL Div: 4.081934690475464e-06\n",
      "====> Epoch: 139 Average loss: 0.0030365707533700124, Recon Loss: 0.0030357739755085536, KL Div: 3.9837913853781564e-06\n",
      "====> Epoch: 140 Average loss: 0.003024060266358512, Recon Loss: 0.0030232678140912736, KL Div: 3.9622017315455845e-06\n",
      "====> Epoch: 141 Average loss: 0.003018843242100307, Recon Loss: 0.0030180596453802926, KL Div: 3.917809043611799e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 142 Average loss: 0.0030167978831699914, Recon Loss: 0.003016047988619123, KL Div: 3.749417407172067e-06\n",
      "====> Epoch: 143 Average loss: 0.003019754137311663, Recon Loss: 0.0030189941780907766, KL Div: 3.799779074532645e-06\n",
      "====> Epoch: 144 Average loss: 0.003020594733101981, Recon Loss: 0.0030198542731148855, KL Div: 3.702291420527867e-06\n",
      "====> Epoch: 145 Average loss: 0.0030132045916148593, Recon Loss: 0.0030124647787639074, KL Div: 3.699051482336862e-06\n",
      "====> Epoch: 146 Average loss: 0.003004043698310852, Recon Loss: 0.0030033140523093088, KL Div: 3.6482087203434536e-06\n",
      "====> Epoch: 147 Average loss: 0.0029970909016472955, Recon Loss: 0.0029963756118501937, KL Div: 3.5764873027801515e-06\n",
      "====> Epoch: 148 Average loss: 0.0030032517399106707, Recon Loss: 0.0030025499037333895, KL Div: 3.5091468266078404e-06\n",
      "====> Epoch: 149 Average loss: 0.0030009967940194267, Recon Loss: 0.0030003022125789096, KL Div: 3.4730264118739536e-06\n",
      "====> Epoch: 150 Average loss: 0.0029937038591929845, Recon Loss: 0.0029930166687284197, KL Div: 3.4358118261609758e-06\n",
      "====> Epoch: 151 Average loss: 0.0029950028317315236, Recon Loss: 0.0029943315471921647, KL Div: 3.356495073863438e-06\n",
      "====> Epoch: 152 Average loss: 0.0029878562007631575, Recon Loss: 0.002987187930515834, KL Div: 3.341325691768101e-06\n",
      "====> Epoch: 153 Average loss: 0.0029974504028047835, Recon Loss: 0.002996798174721854, KL Div: 3.2612723963601248e-06\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lists to store losses for each epoch\n",
    "bavg_losses = []\n",
    "bavg_recon_losses = []\n",
    "bavg_kl_divs = []\n",
    "\n",
    "# Training\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    bavg_loss, bavg_recon_loss, bavg_kl_div = train(epoch, model, optimizer, loss_fn, btrain_loader)\n",
    "    bavg_losses.append(bavg_loss)\n",
    "    bavg_recon_losses.append(bavg_recon_loss)\n",
    "    bavg_kl_divs.append(bavg_kl_div)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(bavg_losses, label='Average Loss')\n",
    "plt.plot(bavg_recon_losses, label='Reconstruction Loss')\n",
    "plt.plot(bavg_kl_divs, label='KL Divergence')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lists to store losses for each epoch\n",
    "ravg_losses = []\n",
    "ravg_recon_losses = []\n",
    "ravg_kl_divs = []\n",
    "\n",
    "# Training\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    ravg_loss, ravg_recon_loss, ravg_kl_div = train(epoch, model, optimizer, loss_fn, rtrain_loader)\n",
    "    ravg_losses.append(ravg_loss)\n",
    "    ravg_recon_losses.append(ravg_recon_loss)\n",
    "    ravg_kl_divs.append(ravg_kl_div)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(ravg_losses, label='Average Loss')\n",
    "plt.plot(ravg_recon_losses, label='Reconstruction Loss')\n",
    "plt.plot(ravg_kl_divs, label='KL Divergence')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lists to store losses for each epoch\n",
    "javg_losses = []\n",
    "javg_recon_losses = []\n",
    "javg_kl_divs = []\n",
    "\n",
    "# Training\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    javg_loss, javg_recon_loss, javg_kl_div = train(epoch, model, optimizer, loss_fn, jtrain_loader)\n",
    "    javg_losses.append(javg_loss)\n",
    "    javg_recon_losses.append(javg_recon_loss)\n",
    "    javg_kl_divs.append(javg_kl_div)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(javg_losses, label='Average Loss')\n",
    "plt.plot(javg_recon_losses, label='Reconstruction Loss')\n",
    "plt.plot(javg_kl_divs, label='KL Divergence')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# see the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# latent space plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get encoded representations (h values) for all data points\n",
    "encoded_representations_forward = []\n",
    "encoded_representations_backward = []\n",
    "encoded_representations_right = []\n",
    "encoded_representations_jelly = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in ftest_loader:\n",
    "        batch_data = batch[0]\n",
    "        batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "        batch_data = batch_data.float()\n",
    "        _, _, h = model.encoder(batch_data)  # Get the encoded representation (h value) directly\n",
    "        encoded_representations_forward.append(h)\n",
    "\n",
    "encoded_representations_forward = torch.cat(encoded_representations_forward, dim=0).numpy()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in btest_loader:\n",
    "        batch_data = batch[0]\n",
    "        batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "        batch_data = batch_data.float()\n",
    "        _, _, h = model.encoder(batch_data)  # Get the encoded representation (h value) directly\n",
    "        encoded_representations_backward.append(h)\n",
    "        \n",
    "encoded_representations_backward = torch.cat(encoded_representations_backward, dim=0).numpy()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in rtest_loader:\n",
    "        batch_data = batch[0]\n",
    "        batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "        batch_data = batch_data.float()\n",
    "        _, _, h = model.encoder(batch_data)  # Get the encoded representation (h value) directly\n",
    "        encoded_representations_right.append(h)\n",
    "        \n",
    "encoded_representations_right = torch.cat(encoded_representations_right, dim=0).numpy()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in jtest_loader:\n",
    "        batch_data = batch[0]\n",
    "        batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "        batch_data = batch_data.float()\n",
    "        _, _, h = model.encoder(batch_data)  # Get the encoded representation (h value) directly\n",
    "        encoded_representations_jelly.append(h)\n",
    "\n",
    "encoded_representations_jelly = torch.cat(encoded_representations_jelly, dim=0).numpy()\n",
    "\n",
    "# # Assuming the encoded representations are 2D, plot the data in 2D space\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(encoded_representations[:, 0], encoded_representations[:, 1], alpha=0.5, s=3)\n",
    "# plt.xlabel('Dimension 1')\n",
    "# plt.ylabel('Dimension 2')\n",
    "# plt.title('2D Visualization of Encoded Representations (h values)')\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_representations_forward[:, 0], encoded_representations_forward[:, 1], alpha=0.5, s=3, label='Forward')\n",
    "plt.scatter(encoded_representations_backward[:, 0], encoded_representations_backward[:, 1], alpha=0.5, s=3, label='Backward')\n",
    "plt.scatter(encoded_representations_right[:, 0], encoded_representations_right[:, 1], alpha=0.5, s=3, label='Right')\n",
    "plt.scatter(encoded_representations_jelly[:, 0], encoded_representations_jelly[:, 1], alpha=0.5, s=3, label='Jelly')\n",
    "\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('2D Visualization of Encoded Representations (h values)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
